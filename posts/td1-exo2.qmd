---
title: "TD 1 - Exercice 2"
lang: fr
author: "Th√©o Leroy"
date: "3 septembre 2025"
params:
  question_courante: 5
format:
  live-html:
    code-background: true
    toc: true
    page-layout: full
webr:
  render-df: gt-interactive
fig-align: center
filters: 
  - custom-callout
editor: 
  mode: source
  markdown: 
    wrap: 72
custom-callout:    
  answer:
    color: "#CCCCCC"
    icon: true
    icon-symbol: "üìù"
    appearance: "default"
    title: "Correction"
---

{{< include ./../_extensions/conditionnal.qmd >}}
{{< include ./../_extensions/r-wasm/live/_knitr.qmd >}}

Les mod√®les suivants sont-ils des mod√®les de r√©gression lin√©aire ? Si
non, peut-on appliquer une transformation pour s‚Äôy ramener ? Pour chaque
mod√®le de r√©gression lin√©aire du type $Y = X\beta + \epsilon$, on
pr√©cisera ce que valent $Y$ , $X$, $\beta$ et $\epsilon\textrm{.}$

## Question 1

On observe $(x_i,y_i)_{i\in [\![1,n ]\!]}$ li√©s th√©oriquement par la
relation $y_i=a+bx_i+\epsilon_i$ pour $i\in [\![1,n ]\!]$, o√π les
variables $\epsilon_i$ sont centr√©es, de variance $\sigma^2$ et non
corr√©l√©es. On d√©sire estimer $a$ et $b$.

::: {.content-visible when-meta="is_answer_print_1"}

::: answer
Le mod√®le $y_{i} = a + b x_{i} + \epsilon_{i}$ est effectivement un
mod√®le de r√©gression lin√©aire car il d√©crit l'esp√©rance d‚Äôune variable
r√©ponse continue $Y$ d‚Äôun √©chantillon al√©atoire de taille $n$ comme
fonction lin√©aire du couple de variables explicatives $(1, X_i)$ :
$\mathbb{E}(Y_i|1,X_i)=a+bX_i$

Le mod√®le donn√© sur les $n$ obervations s'√©crit alors matriciellement
$Y = X\beta + \epsilon$, et nous pouvons identifier les √©l√©ments
suivants :

$$
Y = \underbrace{\begin{pmatrix}
y_1\\
\vdots \\
y_n
\end{pmatrix}}_{\in \mathcal{M}_{n,1}(\mathbb{R})} \qquad
X = \underbrace{\begin{pmatrix}
1 & x_1\\
\vdots & \vdots \\
1 & x_n
\end{pmatrix}}_{\in \mathcal{M}_{n,2}(\mathbb{R})} \qquad
\beta = \underbrace{\begin{pmatrix}
a \\
b
\end{pmatrix}}_{\in \mathcal{M}_{2,1}(\mathbb{R})} \qquad
\epsilon = Y-X\beta=\underbrace{\begin{pmatrix}
\epsilon_1\\
\vdots \\
\epsilon_n
\end{pmatrix}}_{\in \mathcal{M}_{n,1}(\mathbb{R})} 
$$

Il s'agit du mod√®le de r√©gression lin√©aire simple car il n'y a qu'un
seul r√©gresseur et une constante (slide 46).
:::

:::

## Question 2

On observe $(x_i,y_i)_{i\in [\![1,n ]\!]}$ li√©s th√©oriquement par la
relation $y_i=a_1x_i+a_2x_i^2+\epsilon_i$ pour $i\in [\![1,n ]\!]$, o√π
les variables $\epsilon_i$ sont centr√©es, de variance $\sigma^2$ et non
corr√©l√©es. On d√©sire estimer $a_1$ et $a_2$.

::: {.content-visible when-meta="is_answer_print_2"}

::: answer
Il s'agit bien d'un mod√®le de r√©gression lin√©aire car il d√©crit
l'esp√©rance d‚Äôune variable r√©ponse continue $Y$ d‚Äôun √©chantillon
al√©atoire de taille $n$ comme fonction lin√©aire du couple de variables
explicatives $(X_i, X_i^2)$ :
$\mathbb{E}(Y_i|X_i,X_i^2)=a_1X_i+a_2X_i^2$

Le mod√®le donn√© sur les $n$ obervations s'√©crit alors matriciellement
$Y = X\beta + \epsilon$, et nous pouvons identifier les √©l√©ments
suivants :

$$
Y = \underbrace{\begin{pmatrix}
y_1\\
\vdots \\
y_n
\end{pmatrix}}_{\in \mathcal{M}_{n,1}(\mathbb{R})} \qquad
X = \underbrace{\begin{pmatrix}
x_1 & x_1^2\\
\vdots & \vdots \\
x_n & x_n^2
\end{pmatrix}}_{\in \mathcal{M}_{n,2}(\mathbb{R})} \qquad
\beta = \underbrace{\begin{pmatrix}
a_1 \\
a_2
\end{pmatrix}}_{\in \mathcal{M}_{2,1}(\mathbb{R})} \qquad
\epsilon =Y-X\beta= \underbrace{\begin{pmatrix}
\epsilon_1\\
\vdots \\
\epsilon_n
\end{pmatrix}}_{\in \mathcal{M}_{n,1}(\mathbb{R})} 
$$
:::

:::

## Question 3

On rel√®ve pour diff√©rents pays $(i = 1, \dots, n)$ leur production
$P_i$, leur capital $K_i$, leur facteur travail $T_i$ qui sont
th√©oriquement li√©es par la relation de Cobb-Douglas
$P = \alpha_1K^{\alpha_2}T^{\alpha_3}$ . On d√©sire v√©rifier cette
relation et estimer $\alpha_1$, $\alpha_2$ et $\alpha_3$.

::: {.content-visible when-meta="is_answer_print_3"}

::: answer
La relation Cobb-Douglas qui lie la production $P_i$ au capital $K_i$,
et au facteur travail $T_i$ au sein de chaque pays est th√©orique : cela
se traduit plus formellement par une esp√©rance.

$$
\mathbb{E}(P_i | K_i, T_i) = \alpha_1K_i^{\alpha_2}T_i^{\alpha_3}
$$

Ce mod√®le n'est pas un mod√®le de r√©gression lin√©aire car cette esp√©rance
conditionnelle n'est pas une combinaison li√©naire des param√®tres
$\alpha_1$, $\alpha_2$ et $\alpha_3$.

On peut transformer la relation initiale en appliquant la fonction
logarithme.

$$
\ln(P) = \ln(\alpha_1)+\alpha_2\ln(K)+\alpha_3\ln(T)
$$

Le mod√®le statistique s'√©crit donc

$$
\mathbb{E}(\ln(P_i) | K_i, T_i) = \ln(\alpha_1)+\alpha_2\ln(K_i)+\alpha_3\ln(T_i)
$$

Il s'agit bien d'un mod√®le de r√©gression lin√©aire car il d√©crit
l'esp√©rance d‚Äôune variable r√©ponse continue $Y=ln(P)$ d‚Äôun √©chantillon
al√©atoire de taille $n$ comme fonction lin√©aire des variables
explicatives $(1, \ln(K_i), \ln(T_i))$ :
$\mathbb{E}(\ln(P_i)|1, \ln(K_i), \ln(T_i))= \ln(\alpha_1)+\alpha_2\ln(K_i)+\alpha_3\ln(T_i)$

Le mod√®le donn√© sur les $n$ obervations s'√©crit alors matriciellement
$Y = X\beta + \epsilon$, et nous pouvons identifier les √©l√©ments
suivants :

$$
Y = \underbrace{\begin{pmatrix}
\ln(P_1)\\
\vdots \\
\ln(P_n)
\end{pmatrix}}_{\in \mathcal{M}_{n,1}(\mathbb{R})} \qquad
X = \underbrace{\begin{pmatrix}
1 & \ln(K_1) &\ln(T_1)\\
\vdots & \vdots & \vdots \\
1 & \ln(K_n) & \ln(T_n)
\end{pmatrix}}_{\in \mathcal{M}_{n,3}(\mathbb{R})} \qquad
\beta = \underbrace{\begin{pmatrix}
\ln(\alpha_1) \\
\alpha_2 \\
\alpha_3
\end{pmatrix}}_{\in \mathcal{M}_{3,1}(\mathbb{R})} \qquad
\epsilon =Y-X\beta= \underbrace{\begin{pmatrix}
\epsilon_1\\
\vdots \\
\epsilon_n
\end{pmatrix}}_{\in \mathcal{M}_{n,1}(\mathbb{R})} 
$$
:::

:::

## Question 4

Le taux de produit actif $y$ dans un m√©dicament est suppos√© √©voluer au
cours du temps $t$ selon la relation $y = \beta_1e^{-\beta_2t}$. On
dispose des mesures de $n$ taux $y_i$ effectu√©s √† $n$ instants $t_i$. On
d√©sire v√©rifier cette relation et estimer $\beta_1$ et $\beta_2$.

::: {.content-visible when-meta="is_answer_print_4"}

::: answer
Comme dans l'exemple pr√©c√©dent, il s'agit d'une relation th√©orique mais
il y a bien un al√©as implicite. De m√™me la relation,
$\beta_1e^{-\beta_2t}$ n'est pas lin√©aire en les param√®tres.

On peut passer au logarithme afin de lin√©ariser la relation :

$$
\ln(y_i) = \ln(\beta_1)-\beta_2 t_i
$$

Le mod√®le statistique s'√©crit donc

$$
\mathbb{E}(\ln(Y_i) | 1, T_i) = \ln(\beta_1)-\beta_2 t_i
$$

Il s'agit
bien d'un mod√®le de r√©gression lin√©aire car il d√©crit l'esp√©rance d‚Äôune
variable r√©ponse continue $Y=ln(P)$ d‚Äôun √©chantillon al√©atoire de taille
$n$ comme fonction lin√©aire des variables explicatives $(1, T_i)$ :
$\mathbb{E}(\ln(Y_i)|1,  T_i)=  \ln(\beta_1)-\beta_2 T_i$

Le mod√®le donn√© sur les $n$ obervations s'√©crit alors matriciellement
$Y = X\beta + \epsilon$, et nous pouvons identifier les √©l√©ments
suivants :

$$
Y = \underbrace{\begin{pmatrix}
\ln(y_1)\\
\vdots \\
\ln(y_n)
\end{pmatrix}}_{\in \mathcal{M}_{n,1}(\mathbb{R})} \qquad
X = \underbrace{\begin{pmatrix}
1 & T_1\\
\vdots & \vdots \\
1 &  T_n
\end{pmatrix}}_{\in \mathcal{M}_{n,2}(\mathbb{R})} \qquad
\beta = \underbrace{\begin{pmatrix}
\ln(\beta_1) \\
-\beta_2 \\
\end{pmatrix}}_{\in \mathcal{M}_{2,1}(\mathbb{R})} \qquad
\epsilon =Y-X\beta= \underbrace{\begin{pmatrix}
\epsilon_1\\
\vdots \\
\epsilon_n
\end{pmatrix}}_{\in \mathcal{M}_{n,1}(\mathbb{R})} 
$$
:::

:::

## Question 5

M√™me probl√®me que pr√©cedemment mais le mod√®le th√©orique entre les
observations s‚Äô√©crit $y_i = \beta_1e^{-\beta_2t_i}+u_i$,
$i = 1, \dots, n$, o√π les variables $u_i$ sont centr√©es, de variance
$\sigma^2$ et non-corr√©l√©es.

::: {.content-visible when-meta="is_answer_print_5"}

::: answer
Le mod√®le n'est pas lin√©aire car le lien initial n'est pas lin√©aire en
les param√®tres et on ne peut pas appliquer de transformation
logarithmique pour se ramener √† un mod√®le lin√©aire classique car
l'erreur dans le mod√®le initial est additive.
:::

:::
