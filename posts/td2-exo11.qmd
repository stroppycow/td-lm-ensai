---
title: "TD 2 - Exercice 11 - Moindres carrés généralisés"
lang: fr
author: "Théo Leroy"
date: "8 octobre 2024"
format:
  live-html:
    code-background: true
    toc: true
    page-layout: full
editor: 
  markdown: 
    wrap: 72
---

## Question 1

On a :
$\Sigma = \textrm{diag}\left(\sigma_{1}^{2}, \ldots, \sigma_{n}^{2}\right)$

Il s’agit d’une matrice diagonale dont les éléments sur la diagonale
sont les valeurs du vecteur
$\left(\sigma_{1}^{2}, \ldots, \sigma_{n}^{2}\right)$.


## Question 2

L'estimateur des MCO vaut
$\hat{\beta}=\left(X^{\prime} X\right)^{-1} X^{\prime} Y$, d'où

$$
\begin{align*}
\mathbb{E}(\hat{\beta})&=\mathbb{E}\left(\left(X^{\prime} X\right)^{-1} X^{\prime} Y(X\beta + \varepsilon)\right) \\
&= \beta + \left(X^{\prime} X\right)^{-1} X^{\prime}\mathbb{E}(\varepsilon) \\
&=\beta
\end{align*}
$$

par linéarité de l'espérance et car $\mathbb{E}(\varepsilon)=0$

Par ailleurs,

$$
\begin{align*}
\mathbb{V}(\hat{\beta})&=
\left(X^{\prime} X\right)^{-1} X^{\prime} \mathbb{V}(Y) X\left(X^{\prime} X\right)^{-1} \\
&= \left(X^{\prime} X\right)^{-1} X^{\prime} \mathbb{V}(X\beta + \varepsilon) X\left(X^{\prime} X\right)^{-1} \\
&= \left(X^{\prime} X\right)^{-1} X^{\prime} \mathbb{V}(\varepsilon) X\left(X^{\prime} X\right)^{-1} \\
&=\left(X^{\prime} X\right)^{-1} X^{\prime} \Sigma X\left(X^{\prime} X\right)^{-1}
\end{align*}
$$


## Question 3

On cherche à minimiser la fonction
$g: \beta \mapsto \left\|Y-X\beta \right\|_{\Sigma}$ comme
$x \mapsto x^2$ est strictement croissante sur $\mathbb{R_+}$ cela
revient à minimiser
$f: \beta \mapsto \left\|Y-X\beta \right\|_{\Sigma}^2$

D'après les définitions posées dans l'énnoncé, $$
\begin{aligned}
f(\beta) & =(Y-X \beta)^{\prime} \Sigma^{-1}(Y-X \beta) \\
& =Y^{\prime} \Sigma^{-1} Y-Y^{\prime} \Sigma^{-1} X \beta-\beta^{\prime} X^{\prime} \Sigma^{-1} Y+\beta^{\prime} X^{\prime} \Sigma^{-1} X \beta .
\end{aligned}
$$

Lorsqu'on calcule le gradient, on a :
$$ \nabla f(\beta)=-2 X^{\prime} \Sigma^{-1} Y+2 X^{\prime} \Sigma^{-1} X \beta$$

Cette quantité s'annule en $\hat{\beta}_G$ qui est donc la solution
recherchée.

On en déduit : $$
\begin{align*}
\mathbb{E}\left(\hat{\beta}_G\right)&=\left(X^{\prime} \Sigma^{-1} X\right)^{-1} X^{\prime} \Sigma^{-1} \mathbb{E}(Y) \\
&=\left(X^{\prime} \Sigma^{-1} X\right)^{-1} X^{\prime} \Sigma^{-1} X \beta \\
&=\beta
\end{align*}
$$

et $$
\begin{align*}
\mathbb{V}\left(\hat{\beta}_G\right) & =\left(X^{\prime} \Sigma^{-1} X\right)^{-1} X^{\prime} \Sigma^{-1} \mathbb{V}(Y) \Sigma^{-1} X\left(X^{\prime} \Sigma^{-1} X\right)^{-1} \\
& =\left(X^{\prime} \Sigma^{-1} X\right)^{-1} X^{\prime} \Sigma^{-1} \Sigma \Sigma^{-1} X\left(X^{\prime} \Sigma^{-1} X\right)^{-1} \\
& =\left(X^{\prime} \Sigma^{-1} X\right)^{-1} 
\end{align*}
$$

::: {.content-hidden when-format="html"}


## Question 4

Par définition
$$\operatorname{Cov}\left(\hat{\beta}, \hat{\beta}_G\right)=\mathbb{E}\left((\hat{\beta}-\mathbb{E}(\hat{\beta}))\left(\hat{\beta}_G-\mathbb{E}\left(\hat{\beta}_G\right)\right)^{\prime}\right)$$

Or
$$\hat{\beta}-\mathbb{E}(\hat{\beta})=\hat{\beta}-\beta=\left(X^{\prime} X\right)^{-1} X^{\prime}(X\beta+\varepsilon)-\beta=\left(X^{\prime} X\right)^{-1} X^{\prime} \varepsilon$$

De même
$$\hat{\beta}_G-\mathbb{E}\left(\hat{\beta}_G\right)=\left(X^{\prime} \Sigma^{-1} X\right)^{-1} X^{\prime} \Sigma^{-1} \varepsilon$$
Ainsi, puisque
$$\mathbb{E}\left(\varepsilon \varepsilon^{\prime}\right)=\mathbb{V}(\varepsilon)=\Sigma
$$ car $\varepsilon$ est centrée $$
\begin{align*}
\operatorname{Cov}\left(\hat{\beta}, \hat{\beta}_G\right) & =\mathbb{E}\left(\left(X^{\prime} X\right)^{-1} X^{\prime} \epsilon \epsilon^{\prime} \Sigma^{-1} X\left(X^{\prime} \Sigma^{-1} X\right)^{-1}\right) \\
& =\left(X^{\prime} X\right)^{-1} X^{\prime} \mathbb{E}\left(\epsilon \epsilon^{\prime}\right) \Sigma^{-1} X\left(X^{\prime} \Sigma^{-1} X\right)^{-1} \\
& =\left(X^{\prime} X\right)^{-1} X^{\prime} \Sigma \Sigma^{-1} X\left(X^{\prime} \Sigma^{-1} X\right)^{-1} \\
& =\left(X^{\prime} \Sigma^{-1} X\right)^{-1} \\
&=\mathbb{V}\left(\hat{\beta}_G\right) 
\end{align*}
$$

On peut conclure comme dans la preuve du théorème de Gauss-Markov : pour
tout $u \in \mathbb{R}^p$, l'égalité précédente implique que
$\operatorname{Cov}\left(u^{\prime} \hat{\beta}, u^{\prime} \hat{\beta}_G\right)=\mathbb{V}\left(u^{\prime} \hat{\beta}_G\right)$.
Or par Cauchy-Schwartz : $$
\operatorname{Cov}\left(u^{\prime} \hat{\beta}, u^{\prime} \hat{\beta}_G\right) \leq \sqrt{\mathbb{V}\left(u^{\prime} \hat{\beta}\right) \mathbb{V}\left(u^{\prime} \hat{\beta}_G\right)}
$$

Ce qui signifie par l’égalité précédente que pour tout
$u \in \mathbb{R}^p$

$$
\mathbb{V}\left(u^{\prime} \hat{\beta}_G \right) \leq \sqrt{\mathbb{V}\left(u^{\prime} \hat{\beta}\right) \mathbb{V}\left(u^{\prime} \hat{\beta}_G\right)}
$$
:::
