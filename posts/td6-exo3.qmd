---
title: "TD 6 - Exercice 3 - Le mod√®le logistique est naturel (bis)"
lang: fr
author: "Th√©o Leroy"
date: "14 novembre 2025"
params:
  question_courante: 0
format:
  live-html:
    code-background: true
    toc: true
    page-layout: full
editor: 
  mode: source
  markdown: 
    wrap: 72
fig-align: center
filters: 
  - custom-callout
custom-callout:    
  answer:
    color: "#CCCCCC"
    icon: true
    icon-symbol: "üìù"
    appearance: "default"
    title: "Correction"
---

{{< include ./../_extensions/conditionnal.qmd >}}
{{< include ./../_extensions/r-wasm/live/_knitr.qmd >}}


**Pr√©ambule :** 

L‚Äôentropie est une quantit√© qu‚Äôon trouve en thermodynamique pour mesurer l‚Äô√©tat de 
d√©sordre ou d‚Äôal√©a d‚Äôun syst√®me. Dans le m√™me esprit, on la trouve √©galement
en th√©orie de l‚Äôinformation et en probabilit√© pour quantifier le d√©sordre ou la 
quantit√© d‚Äôal√©a qu‚Äôint√®gre une loi de probabilit√©. Un syst√®me physique a tendance
√† √©voluer naturellement vers un √©tat d‚Äôentropie maximale. Suivant ce principe,
il est naturel, pour d√©crire une exp√©rience al√©atoire donn√©e, de choisir des
lois de probabilit√© qui maximisent l‚Äôentropie. C‚Äôest ce principe que nous
allons appliquer pour chercher √† choisir au mieux $\mathbb{P}(Y = 1)$ lorsque 
$Y$ est binaire.

Math√©matiquement, √©tant donn√© $Y$ une variable binaire et 
$p = \mathbb{P}(Y = 1)$, l‚Äôentropie de la loi de $Y$ vaut

$$
-p\log\left(p\right)-(1-p)\log\left(1-p \right)
$$

L‚Äôentropie d‚Äôun vecteur de variables binaires ind√©pendantes 
$\left(Y_1,, \dots, Y_n \right)$ est simplement la
somme des entropies individuelles.

## Question 1

Sans aucune source de contrainte, quelle est la loi d‚Äôentropie maximale d‚Äôune variable
binaire ?


::: {.content-visible when-meta="is_answer_print_1"}

::: answer
On maximise l'entropie $$H(p) = -p \log(p)- (1-p)\log(1-p)$$

Pour se faire, on annule la d√©riv√©e en $p$.

$$
\begin{align*}
H'(p)=-\log(p)-1+\log(1-p)+1=\log\left(\frac{1-p}{p}\right)=0 
&\Longleftrightarrow \frac{1-p}{p} = 1 \\
&\Longleftrightarrow 1-p = p \\
&\Longleftrightarrow p = \frac{1}{2}
\end{align*}
$$

Il s'agit bien d'un maximum car l'entropie est une fonction concave de
$p$ (la d√©riv√©e seconde est n√©gative).

Le r√©sultat est naturel : la loi la plus "al√©atoire" possible est la loi
uniforme qui donne la m√™me masse √† 0 et √† 1.

:::

:::

## Question 2

Supposons √† pr√©sent qu‚Äôon dispose d‚Äôun √©chantillon de $n$ couples $(Y_i,X_i)$ o√π $X_i$
est une variable al√©atoire dans $\mathbb{R}^d$.
On note $p_i(x_i) = \mathbb{P}\left(Y_i = 1 | X_i = x_i\right)$, $i = 1, \dots n$.
A priori, sans utiliser aucune information contenue dans l‚Äô√©chantillon, que valent les
$p_i(x_i)$ qui maximisent l‚Äôentropie ?

::: {.content-visible when-meta="is_answer_print_2"}

::: answer

Il faut trouver les $p_i\left(x_i\right)$ qui maximisent

$$
\sum\limits_{i=1}^n-p_i\left(x_i\right) \log \left(p_i\left(x_i\right)\right)-\left(1-p_i\left(x_i\right)\right) \log (1-p_i\left(x_i\right))
$$

On est ramen√© √† la question pr√©c√©dente pour chaque $i$ et la solution
est $p_i\left(x_i\right)=1 / 2$ pour tout $i$.

:::

:::

## Question 3

On souhaite trouver les $p_i(x_i)$ qui maximisent l‚Äôentropie tout en √©tant coh√©rentes
avec les observations. Cela revient √† inclure des contraintes sur les $p_i(x_i)$ possibles.
On choisit les contraintes :

$$
\sum\limits_{i=1}^n y_ix_i = \sum\limits_{i=1}^n p_i(x_i)x_i
$$

(Puisque $x_i$ est un vecteur de taille $d$, il s‚Äôagit bien d‚Äôun syst√®me de $d$ contraintes).

Ces contraintes sont assez naturelles : on souhaite que la moyenne des $x_i$ des individus
du groupe positif $(y_i = 1)$ coincide avec la moyenne des $x_i$ pond√©r√©e par la
probabilit√© que $y_i$ vale 1. En particulier (pour la variable constante 1), on souhaite
que la proportion des $y_i = 1$ coincide avec la somme des probabilit√©s.

Trouver les $p_i(x_i)$ qui maximisent l‚Äôentropie tout en satisfaisant
les contraintes pr√©c√©dentes. On pourra donner la solution √† une
constante (vectorielle) inconnue pr√®s.

::: {.content-visible when-meta="is_answer_print_3"}

::: answer

On introduit $d$ multiplicateurs de Lagrange
$\lambda_1, \ldots, \lambda_d$. Le probl√®me d'optimisation revient √†
trouver $\left(p_1(x_1), \dots,p_d(x_d)\right)$ qui maximisent le Lagrangien

$$
L=\sum_{i=1}^n\left[-p_i(x_i) \log \left(p_i(x_i)\right)-\left(1-p_i(x_i)\right) \log \left(1-p_i(x_i)\right)\right]+\lambda \cdot\left(\sum_{i=1}^n p_i x_i-\sum_{i=1}^n y_i x_i\right)
$$

o√π $\lambda=\left(\lambda_1, \ldots, \lambda_d\right)$. Il convient
d'annuler les d√©riv√©es partielles de $L$ par rapport √† chaque $p_i(x_i)$
et √† chaque $\lambda_i$ pour obtenir la solution. Par rapport √† chaque
$p_i(x_i)$, cela donne

$$
\frac{\partial L}{\partial p_i(x_i)}=-\log \left(p_i(x_i)\right)-1+\log \left(1-p_i(x_i)\right)+1+\lambda \cdot x_i
$$

qui s'annule en
$$
p_i(x_i)=\frac{1}{1+e^{-\lambda \cdot x_i}}=\frac{e^{\lambda \cdot x_i}}{1+e^{\lambda \cdot x_i}}
$$

Il reste √† trouver $\lambda$. Il doit √™tre tel que les contraintes
souhait√©es soient satisfaites (ce que l'on obtient en d√©rivant le
Lagrangien par rapport √† chaque composante de $\lambda)$, c'est √† dire :

$$
\sum_{i=1}^n\left(\frac{e^{\lambda \cdot x_i}}{1+e^{\lambda \cdot x_i}}-y_i\right) \cdot x_i=0 .
$$

:::

:::

## Question 4

Quel rapport peut-on faire avec la r√©gression logistique ?

::: {.content-visible when-meta="is_answer_print_4"}

::: answer

On constate que la forme de $p_i\left(x_i\right)$ est
$p_i\left(x_i\right)=\operatorname{logit}^{-1}\left(\lambda \cdot x_i\right)$,
qui est exactement celle d'un mod√®le de r√©gression logistique, o√π le
param√®tre $\lambda$ est solution des √©quations de vraisemblance. Le
point de vue pr√©c√©dent (maximiser l'entropie sous contrainte d'une
attache aux donn√©es) motive donc l'utilisation du mod√®le logistique pour
tout $x$ (et non seulement pour les $x_i$ de l'√©chantillon) :

$$
p(x)=\mathbb{P}(Y=1 \mid X=x)=\operatorname{logit}^{-1}(\lambda \cdot x)
$$

:::

:::
