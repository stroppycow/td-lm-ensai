---
title: "TD 1 - Exercice 1"
lang: fr
author: "Th√©o Leroy"
date: "3 septembre 2025"
params:
  question_courante: 3
format:
  live-html:
    code-background: true
    toc: true
    page-layout: full
webr:
  render-df: gt-interactive
fig-align: center
filters: 
  - diagram
  - custom-callout
editor: 
  mode: source
  markdown: 
    wrap: 72
diagram:
  engine:
    tikz:
      execpath: lualatex
      header-includes:
        - '\usepackage{amsmath}'
        - '\usepackage{amssymb}'
        - '\usepackage{tkz-tab}'
        - '\usepackage{fontspec}'
custom-callout:    
  answer:
    color: "#CCCCCC"
    icon: true
    icon-symbol: "üìù"
    appearance: "default"
    title: "Correction"
---

{{< include ./../_extensions/conditionnal.qmd >}}
{{< include ./../_extensions/r-wasm/live/_knitr.qmd >}}

Soit $z_1, \dots, z_n$ des observations d'une variable $Z$.

## Question 1

D√©terminer la valeur $\hat{m}$ qui minimise la distance quadratique aux
donn√©es
$$
S(m) = \sum\limits_{i=1}^n \left( z_i-m \right)^2
$$

::: {.content-visible when-meta="is_answer_print_1"}

::: answer
La fonction $S$ est une fonction polynomiale. Elle est donc d√©rivable
sur $\mathbb{R}$. On note $S'$ la fonction d√©riv√©e de $S$. Pour
$m \in \mathbb{R}$, on a : $$S'(m)=\sum\limits_{i=1}^n 2(m-z_i)$$ On
peut ainsi dresser un tableau de variation de la fonction $S$ en
√©tudiant le signe de $S'$.

$$
\begin{align*}
S'(m) < 0 &\Longleftrightarrow \sum\limits_{i=1}^n \left(m-z_i\right) = 0 \Longleftrightarrow nm-\sum\limits_{i=1}^n z_i  < 0 \Longleftrightarrow m < \frac{1}{n} \sum\limits_{i=1}^n z_i \\
S'(m) = 0 &\Longleftrightarrow \sum\limits_{i=1}^n \left(m-z_i\right) = 0 \Longleftrightarrow nm-\sum\limits_{i=1}^n z_i  = 0 \Longleftrightarrow m = \frac{1}{n} \sum\limits_{i=1}^n z_i \\
S'(m) > 0 &\Longleftrightarrow \sum\limits_{i=1}^n \left(m-z_i\right) = 0 \Longleftrightarrow nm-\sum\limits_{i=1}^n z_i  > 0 \Longleftrightarrow m > \frac{1}{n} \sum\limits_{i=1}^n z_i 
\end{align*}
$$

Si on note $\overline{z}=\frac{1}{n}\sum\limits_{i=1}^n z_i$ la moyenne
des observations $z_1, \dots, z_n$.

::: {style="text-align:center;"}
``` tikz
%%| filename: tablev
\begin{tikzpicture} 
\tkzTabInit{$m$ / 1 ,  $S'(m)$ / 1, $S(m)$ / 1.5}{$-\infty$, $\overline{z}$, $+\infty$}
\tkzTabLine{, -, 0, +, }
\tkzTabVar{+/ $+\infty$, -/ $S(\overline{z})$, +/ $+\infty$}
\end{tikzpicture}
```
:::

La valeur $\hat{m}$ qui minimise la distance quadratique $S(m)$ aux
donn√©es est donc la moyenne empirique $\overline{z}$.
:::

:::

## Question 2

La quantit√© $\hat{m}$ correspond en fait √† l‚Äôestimation par moindres
carr√©s ordinaires dans un mod√®le de r√©gression lin√©aire :
$Y = X\beta + \epsilon$. Pr√©ciser ce que valent $Y$ , $X$, $\beta$ et
$\epsilon$.

::: {.content-visible when-meta="is_answer_print_2"}

::: answer
Dans le cas o√π on ne dispose pas d'informations auxiliaires, on peut
mod√©liser lin√©airement √† quel point les observations s'ajustent √† une
constante $m$.

On mod√©lise ainsi $$\forall\ i \in  [\![1, n ]\!], \ z_i=m+\epsilon_i $$

Vectoriellement, cela se traduit par l'expression

$$
\underbrace{\begin{pmatrix}
z_1 \\
\vdots \\
z_n
\end{pmatrix}}_{Y}
=
\underbrace{\begin{pmatrix}
1\\
\vdots \\
1
\end{pmatrix}}_{X}
\underbrace{m}_{\beta}+
\underbrace{\begin{pmatrix}
\epsilon_1\\
\vdots \\
\epsilon_n
\end{pmatrix}}_{\epsilon}
$$

Les moindres carr√©s ordinaires (MCO) consistent √† trouver la valeur de
$\beta$ qui minimise la quantit√©
$$
\|Y-X\beta\|^2  = \sum\limits_{i=1}^n \left(z_i-\beta\right)^2=S(\beta)
$$

Ainsi, la quantit√© $\hat{m}$ calcul√©e √† la question pr√©c√©dente
correspond bien √† l'estimation par les moindres carr√©s ordinaires de ce
mod√®le de regression lin√©aire avec uniquement une constante.
:::

:::

## Question 3

Retrouver le r√©sultat de la premi√®re question √† partir de la formule
g√©n√©rale de l'estimateur des moindres carr√©s ordinaires :
$\hat{\beta}=(X'X)^{-1}X'Y$.

::: {.content-visible when-meta="is_answer_print_3"}

::: answer
On calcule $\hat{\beta}$ √©tape par √©tape :

$$
\begin{align*}
X &= \underbrace{\begin{pmatrix}
1\\
\vdots \\
1
\end{pmatrix}}_{\in \mathcal{M}_{n,1}(\mathbb{R})} \\
X' &= \underbrace{\begin{pmatrix}
1 &
\dots &
1
\end{pmatrix}}_{\in \mathcal{M}_{1,n}(\mathbb{R})} \\
X'X &= \begin{pmatrix}
1 &
\dots &
1
\end{pmatrix}\begin{pmatrix}
1\\
\vdots \\
1
\end{pmatrix}=\underbrace{n}_{\in \mathcal{M}_{1,1}(\mathbb{R})} \\
(X'X)^{-1} &= \underbrace{\frac{1}{n}}_{\in \mathcal{M}_{1,1}(\mathbb{R})} \\
X'Y &= \begin{pmatrix}
1 &
\dots &
1
\end{pmatrix}\begin{pmatrix}
z_1\\
\vdots \\
z_n
\end{pmatrix}=\underbrace{\sum\limits_{i=1}^n z_i}_{\in \mathcal{M}_{1,1}(\mathbb{R})} \\
\hat{\beta}&=(X'X)^{-1}X'Y=\frac{1}{n}\sum\limits_{i=1}^n z_i = \overline{z}
\end{align*}
$$

On retrouve ainsi le r√©sultat de la premi√®re question
$\hat{m}=\overline{z}$.

:::

:::
