---
title: "TD 2 - Exercice 12 - Moindres carr√©s g√©n√©ralis√©s"
lang: fr
author: "Th√©o Leroy"
date: "17 octobre 2025"
params:
  question_courante: 0
format:
  live-html:
    code-background: true
    toc: true
    page-layout: full
editor: 
  mode: source
  markdown: 
    wrap: 72
fig-align: center
filters: 
  - custom-callout
custom-callout:    
  answer:
    color: "#CCCCCC"
    icon: true
    icon-symbol: "üìù"
    appearance: "default"
    title: "Correction"
---

{{< include ./../_extensions/conditionnal.qmd >}}
{{< include ./../_extensions/r-wasm/live/_knitr.qmd >}}

Soit un mod√®le de r√©gression lin√©aire multiple

$$
Y=X\beta+\epsilon
$$

o√π  $\beta \in \mathbb{R}^p$, $X$ est une matrice de taille $n \times p$ et $\epsilon$ est un vecteur al√©atoire de taille $n$,
centr√©. On consid√®re ici la situation o√π les variables $\epsilon_i$ ne sont plus homosc√©dastiques et
non-corr√©l√©s, mais de fa√ßon g√©n√©rale $V(\epsilon) = \Sigma$ o√π $\Sigma$ est une matrice inversible. On suppose
dans cet exercice que $\Sigma$ est connue (il conviendra dans la pratique de l‚Äôestimer).

## Question 1

Pr√©ciser la matrice $\Sigma$ lorsque les variables $\epsilon_i$ sont non-corr√©l√©s mais h√©t√©rosc√©dastiques
de variance $\sigma_i^2$ $(i = 1, \dots, n)$.


::: {.content-visible when-meta="is_answer_print_1"}

::: answer
On a :
$\Sigma = \textrm{diag}\left(\sigma_{1}^{2}, \ldots, \sigma_{n}^{2}\right)$

Il s‚Äôagit d‚Äôune matrice diagonale dont les √©l√©ments sur la diagonale
sont les valeurs du vecteur
$\left(\sigma_{1}^{2}, \ldots, \sigma_{n}^{2}\right)$.

:::

:::

## Question 2

D√©terminer l‚Äôesp√©rance et la variance de l‚Äôestimateur $\hat{\beta}$ des moindres carr√©s ordinaires
(dans le cas g√©n√©ral d‚Äôune matrice $\Sigma$ quelconque).


::: {.content-visible when-meta="is_answer_print_2"}

::: answer

L'estimateur des MCO vaut
$\hat{\beta}=\left(X^{\prime} X\right)^{-1} X^{\prime} Y$, d'o√π

$$
\begin{align*}
\mathbb{E}(\hat{\beta})&=\mathbb{E}\left(\left(X^{\prime} X\right)^{-1} X^{\prime} Y(X\beta + \varepsilon)\right) \\
&= \beta + \left(X^{\prime} X\right)^{-1} X^{\prime}\mathbb{E}(\varepsilon) \\
&=\beta
\end{align*}
$$

par lin√©arit√© de l'esp√©rance et car $\mathbb{E}(\varepsilon)=0$

Par ailleurs,

$$
\begin{align*}
\mathbb{V}(\hat{\beta})&=
\left(X^{\prime} X\right)^{-1} X^{\prime} \mathbb{V}(Y) X\left(X^{\prime} X\right)^{-1} \\
&= \left(X^{\prime} X\right)^{-1} X^{\prime} \mathbb{V}(X\beta + \varepsilon) X\left(X^{\prime} X\right)^{-1} \\
&= \left(X^{\prime} X\right)^{-1} X^{\prime} \mathbb{V}(\varepsilon) X\left(X^{\prime} X\right)^{-1} \\
&=\left(X^{\prime} X\right)^{-1} X^{\prime} \Sigma X\left(X^{\prime} X\right)^{-1}
\end{align*}
$$

:::

:::

## Question 3


Pour $S \in R^n$ et $T \in R^n$, on d√©finit le produit scalaire entre $S$ et $T$ associ√© √† la
matrice $\Sigma^{-1}$ par $S'\Sigma^{-1}T$, et donc la norme de $T$ associ√©e √† $\Sigma^{-1}$ est $\left\| T\right\|^2_{\Sigma}=T'\Sigma^{-1}T$.


Montrer que la forme explicite de l‚Äôestimateur $\hat{\beta}_G$ des moindres carr√©s g√©n√©ralis√©s
d√©fini comme le minimiseur de $\left\| Y-X\beta\right\|^2_{\Sigma}=T'\Sigma^{-1}T$ est


$$
\hat{\beta}_G = (X'\Sigma^{-1}X)^{-1}X'\Sigma^{-1}Y
$$

En d√©duire son esp√©rance et sa variance.


::: {.content-visible when-meta="is_answer_print_3"}

::: answer

On cherche √† minimiser la fonction
$g: \beta \mapsto \left\|Y-X\beta \right\|_{\Sigma}$ comme
$x \mapsto x^2$ est strictement croissante sur $\mathbb{R_+}$ cela
revient √† minimiser
$f: \beta \mapsto \left\|Y-X\beta \right\|_{\Sigma}^2$

D'apr√®s les d√©finitions pos√©es dans l'√©nnonc√©,
$$
\begin{aligned}
f(\beta) & =(Y-X \beta)^{\prime} \Sigma^{-1}(Y-X \beta) \\
& =Y^{\prime} \Sigma^{-1} Y-Y^{\prime} \Sigma^{-1} X \beta-\beta^{\prime} X^{\prime} \Sigma^{-1} Y+\beta^{\prime} X^{\prime} \Sigma^{-1} X \beta .
\end{aligned}
$$

Lorsqu'on calcule le gradient, on a :
$$ \nabla f(\beta)=-2 X^{\prime} \Sigma^{-1} Y+2 X^{\prime} \Sigma^{-1} X \beta$$

Cette quantit√© s'annule en $\hat{\beta}_G$ qui est donc la solution
recherch√©e.

On en d√©duit :
$$
\begin{align*}
\mathbb{E}\left(\hat{\beta}_G\right)&=\left(X^{\prime} \Sigma^{-1} X\right)^{-1} X^{\prime} \Sigma^{-1} \mathbb{E}(Y) \\
&=\left(X^{\prime} \Sigma^{-1} X\right)^{-1} X^{\prime} \Sigma^{-1} X \beta \\
&=\beta
\end{align*}
$$

et

$$
\begin{align*}
\mathbb{V}\left(\hat{\beta}_G\right) & =\left(X^{\prime} \Sigma^{-1} X\right)^{-1} X^{\prime} \Sigma^{-1} \mathbb{V}(Y) \Sigma^{-1} X\left(X^{\prime} \Sigma^{-1} X\right)^{-1} \\
& =\left(X^{\prime} \Sigma^{-1} X\right)^{-1} X^{\prime} \Sigma^{-1} \Sigma \Sigma^{-1} X\left(X^{\prime} \Sigma^{-1} X\right)^{-1} \\
& =\left(X^{\prime} \Sigma^{-1} X\right)^{-1} 
\end{align*}
$$

:::

:::

## Question 4

Montrer que la matrice de covariance entre $\hat{\beta}$ et $\hat{\beta}_G$ est √©gale √† la matrice de variancecovariance
de $\hat{\beta}_G$. En d√©duire que $\hat{\beta}_G$ est meilleur que $\hat{\beta}$ au sens du co√ªt quadratique.

::: {.content-visible when-meta="is_answer_print_4"}

::: answer

Par d√©finition
$$
\operatorname{Cov}\left(\hat{\beta}, \hat{\beta}_G\right)=\mathbb{E}\left((\hat{\beta}-\mathbb{E}(\hat{\beta}))\left(\hat{\beta}_G-\mathbb{E}\left(\hat{\beta}_G\right)\right)^{\prime}\right)
$$

Or
$$
\hat{\beta}-\mathbb{E}(\hat{\beta})=\hat{\beta}-\beta=\left(X^{\prime} X\right)^{-1} X^{\prime}(X\beta+\varepsilon)-\beta=\left(X^{\prime} X\right)^{-1} X^{\prime} \varepsilon
$$

De m√™me
$$
\hat{\beta}_G-\mathbb{E}\left(\hat{\beta}_G\right)=\left(X^{\prime} \Sigma^{-1} X\right)^{-1} X^{\prime} \Sigma^{-1} \varepsilon
$$

Ainsi, puisque

$$
\mathbb{E}\left(\varepsilon \varepsilon^{\prime}\right)=\mathbb{V}(\varepsilon)=\Sigma
$$

car $\varepsilon$ est centr√©e

$$
\begin{align*}
\operatorname{Cov}\left(\hat{\beta}, \hat{\beta}_G\right) & =\mathbb{E}\left(\left(X^{\prime} X\right)^{-1} X^{\prime} \epsilon \epsilon^{\prime} \Sigma^{-1} X\left(X^{\prime} \Sigma^{-1} X\right)^{-1}\right) \\
& =\left(X^{\prime} X\right)^{-1} X^{\prime} \mathbb{E}\left(\epsilon \epsilon^{\prime}\right) \Sigma^{-1} X\left(X^{\prime} \Sigma^{-1} X\right)^{-1} \\
& =\left(X^{\prime} X\right)^{-1} X^{\prime} \Sigma \Sigma^{-1} X\left(X^{\prime} \Sigma^{-1} X\right)^{-1} \\
& =\left(X^{\prime} \Sigma^{-1} X\right)^{-1} \\
&=\mathbb{V}\left(\hat{\beta}_G\right) 
\end{align*}
$$

On peut conclure comme dans la preuve du th√©or√®me de Gauss-Markov : pour
tout $u \in \mathbb{R}^p$, l'√©galit√© pr√©c√©dente implique que
$\operatorname{Cov}\left(u^{\prime} \hat{\beta}, u^{\prime} \hat{\beta}_G\right)=\mathbb{V}\left(u^{\prime} \hat{\beta}_G\right)$.
Or par Cauchy-Schwartz :
$$
\operatorname{Cov}\left(u^{\prime} \hat{\beta}, u^{\prime} \hat{\beta}_G\right) \leq \sqrt{\mathbb{V}\left(u^{\prime} \hat{\beta}\right) \mathbb{V}\left(u^{\prime} \hat{\beta}_G\right)}
$$

Ce qui signifie par l‚Äô√©galit√© pr√©c√©dente que pour tout
$u \in \mathbb{R}^p$

$$
\mathbb{V}\left(u^{\prime} \hat{\beta}_G \right) \leq \sqrt{\mathbb{V}\left(u^{\prime} \hat{\beta}\right) \mathbb{V}\left(u^{\prime} \hat{\beta}_G\right)}
$$
:::

:::
