---
title: "TD 2 - Exercice 6 - Effet de la multicolinéarité"
lang: fr
author: "Théo Leroy"
date: "10 octobre 2025"
format:
  live-html:
    code-background: true
    toc: true
    page-layout: full
editor: 
  mode: source
  markdown: 
    wrap: 72
fig-align: center
filters: 
  - diagram
  - custom-callout
diagram:
  engine:
    tikz:
      execpath: lualatex
      header-includes:
        - '\usepackage{tkz-tab}'
custom-callout:    
  answer:
    color: "#CCCCCC"
    appearance: "minimal"
    title: ""
---

On considère un modèle à deux variables explicatives, supposées centrées. De l’estimation sur $n$ individus, on a obtenu les matrices $X'X$ et $X'Y$ suivantes :

$$
X'X=\begin{pmatrix}
200 & 150 \\
150 & 113
\end{pmatrix} \qquad
X'Y=\begin{pmatrix}
350 \\
263
\end{pmatrix}\textrm{.}
$$

La suppression d'une observation a modifié ces matrices de la façon suivante :
$$
X'X=\begin{pmatrix}
199 & 149 \\
149 & 112
\end{pmatrix} \qquad
X'Y=\begin{pmatrix}
347.5 \\
261.5
\end{pmatrix}\textrm{.}
$$

## Question 1

Calculer les coefficients estimés de la régression dans les deux cas.

::: callout-tip
## Formule d'inversion des matrices $2 \times 2$

Soit $M \in \mathcal{M}_2\left(\mathbb{R} \right)$ avec $$
M = \begin{pmatrix}
a & b \\
c & d \\
\end{pmatrix}
$$ $M$ est inversible si et seulement si $\det(M) = ad-cb \neq 0$ et
dans ce cas $$
M^{-1}=\frac{1}{ab-cb}\begin{pmatrix}
d & -b \\
-c & a \\
\end{pmatrix}
$$
:::

::: answer

On estime les coefficients par les moindres carrés ordinaires dans ces
deux cas.

**Cas 1 : Estimation sur** $n$ individus

$$
\begin{align*}
\left( X^{'} X \right)^{-1}&=\frac{1}{200\times113 - 150^2 }\begin{pmatrix}
113 & -150 \\
-150 & 200 \\
\end{pmatrix} \\
&=
\frac{1}{22600 - 22500 }\begin{pmatrix}
113 & -150 \\
-150 & 200 \\
\end{pmatrix} \\
&=\begin{pmatrix}
1,13 & -1,5 \\
-1,5 & 2 \\
\end{pmatrix}
\end{align*}
$$ Ainsi, $$
\widehat{\beta}_n = \left( X^{'} X \right)^{-1}X^{'}Y = \begin{pmatrix}
1,13 & -1,5 \\
-1,5 & 2 \\
\end{pmatrix}\times\begin{pmatrix}
350 \\
263
\end{pmatrix} =\begin{pmatrix} 
395,5 -394,5 \\
(-525)+526
\end{pmatrix} =
\begin{pmatrix}
1 \\
1
\end{pmatrix}
$$

**Cas 2 : Estimation sur** $n-1$ individus

$$
\begin{align*}
\left( X^{'} X \right)^{-1}&=\frac{1}{199\times 112 - 149^2 }\begin{pmatrix}
112 & -149 \\
-149 & 199 \\
\end{pmatrix} \\
&=
\frac{1}{22288 - 22201 }\begin{pmatrix}
112 & -149 \\
-149 & 199 \\
\end{pmatrix} \\
&=\frac{1}{87}\begin{pmatrix}
112 & -149 \\
-149 & 199 \\
\end{pmatrix}
\end{align*}
$$ Ainsi, $$
\widehat{\beta}_{n-1} = \left( X^{'} X \right)^{-1}X^{'}Y = \frac{1}{87}\begin{pmatrix}
112 & -149 \\
-149 & 199 \\
\end{pmatrix}
\times\begin{pmatrix}
347,5 \\
261,5
\end{pmatrix} =\frac{1}{87}\begin{pmatrix} 
38920 -38963,5 \\
(-51777,5)+52038.5
\end{pmatrix} =
\frac{1}{87}\begin{pmatrix}
-43,5 \\
261
\end{pmatrix}= \begin{pmatrix}
-0,5 \\
3
\end{pmatrix}
$$ À première vue, les estimations $\widehat{\beta}_n$ sont assez
différentes $\widehat{\beta}_{n-1}$ alors que les données sont presque
les mêmes.

:::

## Question 2

Calculer le coefficient de corrélation linéaire entre les deux variables explicatives.

::: answer

Calculons le coefficient de corrélation linéaire entre les deux
variables explicatives :

Si on base le calcul sur les $n$ individus initiaux, en notant
$x_{i, 1}$ les valeurs de la première variable et $x_{i, 2}$ les valeurs
de la seconde (de telle sorte que les entrées de la matrice $X$ sont
$\left.x_{i, j}\right):$ $$
\begin{aligned}
\hat{\rho}_n & =\frac{\sum\limits_{i=1}^n\left(x_{i, 1}-\bar{x}_1\right)\left(x_{i, 2}-\bar{x}_2\right)}{\sqrt{\sum\limits_{i=1}^n\left(x_{i, 1}-\bar{x}_1\right)^2 \sum\limits_{i=1}^n\left(x_{i, 2}-\bar{x}_2\right)^2}} \\
& =\frac{\sum\limits_{i=1}^n x_{i, 1} x_{i, 2}}{\sqrt{\sum\limits_{i=1}^n x_{i, 1}^2 \sum\limits_{i=1}^n x_{i, 2}^2}}
\end{aligned}
$$ car $\bar{x}_1=\bar{x}_2=0$ par hypothèse. Or $$
X^{\prime} X=\left(\begin{array}{cc}
\sum\limits_{i=1}^n x_{i, 1}^2 & \sum\limits_{i=1}^n x_{i, 1} x_{i, 2} \\
\sum\limits_{i=1}^n x_{i, 1} x_{i, 2} & \sum\limits_{i=1}^n x_{i, 2}^2
\end{array}\right)
$$

Donc $\hat{\rho}_n=150 / \sqrt{200 \times 113} \approx 0,9979$. Si on
base le calcule sur la seconde situation, c'est à dire sur les $n-1$
individus restants, on obtient de la même manière
$\hat{\rho}_{n-1}=149 / \sqrt{199 \times 112} \approx 0,9980$.

:::

## Question 3

Commenter.

::: answer

Cette situation met en évidence le problème de la multicolinéarité :
lorsque deux variables explicatives sont fortement corrélées, même si la
matrice $X'X$ reste inversible, l'estimateur $\hat{\beta}$ devient très instable, avec une variance très élevée. Concrètement, cela signifie que l’ajout ou le
retrait d’un seul individu peut entraîner une variation radicale de la
valeur de $\hat{\beta}$. C'est ce que l'on observe ici, indépendamment de la taille de l'échantillon, même si $n$ est de l'ordre du million. Une telle
situation est évidemment indésirable sur le plan statistique. Cela nous
incite donc à identifier et à corriger ce problème en pratique. On peut
le détecter, entre autres, grâce au calcul des VIF, et le résoudre en
supprimant par exemple la variable la moins pertinente parmi celles en
cause.

:::