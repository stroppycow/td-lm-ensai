---
title: "TD 2 - Exercice 6 - Effet de la multicolin√©arit√©"
lang: fr
author: "Th√©o Leroy"
date: "10 octobre 2025"
params:
  question_courante: 3
format:
  live-html:
    code-background: true
    toc: true
    page-layout: full
editor: 
  mode: source
  markdown: 
    wrap: 72
fig-align: center
filters: 
  - diagram
  - custom-callout
diagram:
  engine:
    tikz:
      execpath: lualatex
      header-includes:
        - '\usepackage{tkz-tab}'
custom-callout:    
  answer:
    color: "#CCCCCC"
    icon: true
    icon-symbol: "üìù"
    appearance: "default"
    title: "Correction"
---

{{< include ./../_extensions/conditionnal.qmd >}}
{{< include ./../_extensions/r-wasm/live/_knitr.qmd >}}

On consid√®re un mod√®le √† deux variables explicatives, suppos√©es centr√©es. De l‚Äôestimation sur $n$ individus, on a obtenu les matrices $X'X$ et $X'Y$ suivantes :

$$
X'X=\begin{pmatrix}
200 & 150 \\
150 & 113
\end{pmatrix} \qquad
X'Y=\begin{pmatrix}
350 \\
263
\end{pmatrix}\textrm{.}
$$

La suppression d'une observation a modifi√© ces matrices de la fa√ßon suivante :
$$
X'X=\begin{pmatrix}
199 & 149 \\
149 & 112
\end{pmatrix} \qquad
X'Y=\begin{pmatrix}
347.5 \\
261.5
\end{pmatrix}\textrm{.}
$$

## Question 1

Calculer les coefficients estim√©s de la r√©gression dans les deux cas.

::: callout-tip
## Formule d'inversion des matrices $2 \times 2$

Soit $M \in \mathcal{M}_2\left(\mathbb{R} \right)$ avec

$$
M = \begin{pmatrix}
a & b \\
c & d \\
\end{pmatrix}
$$

$M$ est inversible si et seulement si $\det(M) = ad-cb \neq 0$ et
dans ce cas

$$
M^{-1}=\frac{1}{ad-cb}\begin{pmatrix}
d & -b \\
-c & a \\
\end{pmatrix}
$$

:::

::: {.content-visible when-meta="is_answer_print_1"}

::: answer

On estime les coefficients par les moindres carr√©s ordinaires dans ces
deux cas.

**Cas 1 : Estimation sur** $n$ **individus**

$$
\begin{align*}
\left( X^{'} X \right)^{-1}&=\frac{1}{200\times113 - 150^2 }\begin{pmatrix}
113 & -150 \\
-150 & 200 \\
\end{pmatrix} \\
&=
\frac{1}{22600 - 22500 }\begin{pmatrix}
113 & -150 \\
-150 & 200 \\
\end{pmatrix} \\
&=\begin{pmatrix}
1,13 & -1,5 \\
-1,5 & 2 \\
\end{pmatrix}
\end{align*}
$$

Ainsi,

$$
\widehat{\beta}_n = \left( X^{'} X \right)^{-1}X^{'}Y = \begin{pmatrix}
1,13 & -1,5 \\
-1,5 & 2 \\
\end{pmatrix}\times\begin{pmatrix}
350 \\
263
\end{pmatrix} =\begin{pmatrix} 
395,5 -394,5 \\
(-525)+526
\end{pmatrix} =
\begin{pmatrix}
1 \\
1
\end{pmatrix}
$$

**Cas 2 : Estimation sur** $n-1$ **individus**

$$
\begin{align*}
\left( X^{'} X \right)^{-1}&=\frac{1}{199\times 112 - 149^2 }\begin{pmatrix}
112 & -149 \\
-149 & 199 \\
\end{pmatrix} \\
&=
\frac{1}{22288 - 22201 }\begin{pmatrix}
112 & -149 \\
-149 & 199 \\
\end{pmatrix} \\
&=\frac{1}{87}\begin{pmatrix}
112 & -149 \\
-149 & 199 \\
\end{pmatrix}
\end{align*}
$$

Ainsi,

$$
\widehat{\beta}_{n-1} = \left( X^{'} X \right)^{-1}X^{'}Y = \frac{1}{87}\begin{pmatrix}
112 & -149 \\
-149 & 199 \\
\end{pmatrix}
\times\begin{pmatrix}
347,5 \\
261,5
\end{pmatrix} =\frac{1}{87}\begin{pmatrix} 
38920 -38963,5 \\
(-51777,5)+52038.5
\end{pmatrix} =
\frac{1}{87}\begin{pmatrix}
-43,5 \\
261
\end{pmatrix}= \begin{pmatrix}
-0,5 \\
3
\end{pmatrix}
$$

√Ä premi√®re vue, les estimations $\widehat{\beta}_n$ sont assez
diff√©rentes $\widehat{\beta}_{n-1}$ alors que les donn√©es sont presque
les m√™mes.

:::

:::

## Question 2

Sur la base des $n$ individus, calculer le coefficient de corr√©lation lin√©aire entre les
deux variables explicatives.

::: {.content-visible when-meta="is_answer_print_2"}

::: answer

Calculons le coefficient de corr√©lation lin√©aire entre les deux
variables explicatives :

Si on base le calcul sur les $n$ individus initiaux, en notant
$x_{i, 1}$ les valeurs de la premi√®re variable et $x_{i, 2}$ les valeurs
de la seconde (de telle sorte que les entr√©es de la matrice $X$ sont
$\left.x_{i, j}\right):$

$$
\begin{aligned}
\hat{\rho}_n & =\frac{\sum\limits_{i=1}^n\left(x_{i, 1}-\bar{x}_1\right)\left(x_{i, 2}-\bar{x}_2\right)}{\sqrt{\sum\limits_{i=1}^n\left(x_{i, 1}-\bar{x}_1\right)^2 \sum\limits_{i=1}^n\left(x_{i, 2}-\bar{x}_2\right)^2}} \\
& =\frac{\sum\limits_{i=1}^n x_{i, 1} x_{i, 2}}{\sqrt{\sum\limits_{i=1}^n x_{i, 1}^2 \sum\limits_{i=1}^n x_{i, 2}^2}}
\end{aligned}
$$

car $\bar{x}_1=\bar{x}_2=0$ par hypoth√®se. Or

$$
X^{\prime} X=\left(\begin{array}{cc}
\sum\limits_{i=1}^n x_{i, 1}^2 & \sum\limits_{i=1}^n x_{i, 1} x_{i, 2} \\
\sum\limits_{i=1}^n x_{i, 1} x_{i, 2} & \sum\limits_{i=1}^n x_{i, 2}^2
\end{array}\right)
$$

Donc $\hat{\rho}_n=150 / \sqrt{200 \times 113} \approx 0,9979$.

:::

:::

## Question 3

Commenter.

::: {.content-visible when-meta="is_answer_print_3"}

::: answer

Cette situation met en √©vidence le probl√®me de la multicolin√©arit√© :
lorsque deux variables explicatives sont fortement corr√©l√©es, m√™me si la
matrice $X'X$ reste inversible, l'estimateur $\hat{\beta}$ devient tr√®s instable, avec une variance tr√®s √©lev√©e. Concr√®tement, cela signifie que l‚Äôajout ou le
retrait d‚Äôun seul individu peut entra√Æner une variation radicale de la
valeur de $\hat{\beta}$. C'est ce que l'on observe ici, ind√©pendamment de la taille de l'√©chantillon, m√™me si $n$ est de l'ordre du million. Une telle situation est √©videmment ind√©sirable sur le plan statistique. Cela nous
incite donc √† identifier et √† corriger ce probl√®me en pratique. On peut
le d√©tecter, entre autres, gr√¢ce au calcul des VIF, et le r√©soudre en
supprimant par exemple la variable la moins pertinente parmi celles en
cause.

:::

:::
