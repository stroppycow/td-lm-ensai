---
title: "TD 1 - Exercice 5"
lang: fr
author: "Th√©o Leroy"
date: "3 septembre 2025"
params:
  question_courante: 0
format:
  live-html:
    code-background: true
    toc: true
    page-layout: full
webr:
  render-df: gt-interactive
fig-align: center
filters: 
  - custom-callout
editor: 
  mode: source
  markdown: 
    wrap: 72
custom-callout:    
  answer:
    color: "#CCCCCC"
    icon: true
    icon-symbol: "üìù"
    appearance: "default"
    title: "Correction"
---

{{< include ./../_extensions/conditionnal.qmd >}}
{{< include ./../_extensions/r-wasm/live/_knitr.qmd >}}

On se place comme dans l‚Äôexercice pr√©c√©dent dans le cadre d‚Äôun mod√®le de
r√©gression lin√©aire simple. On rappelle que la matrice de design $X$ et
la matrice $(X'X)^{-1}$ valent dans ce cas :

$$
X = \underbrace{\begin{pmatrix}
1 & x_1\\
\vdots & \vdots \\
1 & x_n
\end{pmatrix}}_{\in \mathcal{M}_{n,2}(\mathbb{R})}, \quad (X'X)^{-1}=\frac{1}{\sum\limits_{i=1}^n (x_i-\overline{x})^2}\begin{pmatrix}
\frac{1}{n}\sum\limits_{i=1}^n x_i^2 & -\overline{x} \\
-\overline{x} & 1
\end{pmatrix}
$$

On va examiner quelques exemples de design, c‚Äôest √† dire de r√©partition
des valeurs de $x_1, \dots, x_n$, et v√©rifier la convergence (ou non)
des estimateurs par MCO des param√®tres $\beta_0$ et $\beta_1$ dans
chaque cas.

## Question 1

Rappeler ce que vaut l‚Äôerreur quadratique moyenne de $\hat{\beta}$,
l‚Äôestimateur par MCO de
$\beta = \begin{pmatrix} \beta_0 & \beta_1 \end{pmatrix}'$.

::: {.content-visible when-meta="is_answer_print_1"}

::: answer
L'erreur quadratique moyenne vaut

$$
\textrm{EQM}\left(\widehat{\beta} \right) = \mathbb{E}\left(\left[\widehat{\beta} -\beta \right]^2\right) =  \mathbb{V}\left(\widehat{\beta} \right) + \underbrace{\left(\mathbb{E}\left(\widehat{\beta} -\beta \right)\right)^2}_{=0 \textrm{ car } \widehat{\beta} \textrm{ est sans biais}}=\mathbb{V}\left(\widehat{\beta} \right) = \sigma^2\left(X^{'}X \right)^{-1}
$$
:::

:::

## Question 2

Dans ce premier exemple, on se place dans le cas o√π les observations ont
lieu de fa√ßon r√©guli√®rement espac√©es, et deviennent de plus en plus
nombreuses avec $n$. Quitte √† renormaliser, on suppose ainsi que
$x_i = i$ pour tout $i \in [\![1,n]\!]$.

a.  Donner la limite de la matrice $\mathbb{V}(\hat{\beta})$ lorsque
    $n \to +\infty$.

b.  En d√©duire le comportement asymptotique en moyenne quadratique de
    $\hat{\beta}_0$ et $\hat{\beta}_1$.

::: {.content-visible when-meta="is_answer_print_2"}

::: answer
a.  Dans le cas o√π $x_i=i$ pour tout $i=1, \dots, n$, on:

    $$
    \begin{align*}
    \overline{x} &= \frac{1}{n}\sum\limits_{i=1}^n x_i = \frac{1}{n}\sum\limits_{i=1}^n i =\frac{1}{n}\frac{n(n+1)}{2}=\frac{n+1}{2} \\
    \sum\limits_{i=1}^n x_i^2 &= \sum\limits_{i=1}^n i^2 = \frac{n(n+1)(2n+1)}{6} \\ 
    \sum\limits_{i=1}^n (x_i-\overline{x})^2&=\sum\limits_{i=1}^n x_i^2-n\overline{x}^2=\frac{n(n+1)(2n+1)}{6}-\frac{n(n+1)^2}{4}=\frac{n(n-1)(n+1)}{12} 
    \end{align*}
    $$

    Ainsi,

    $$
    \mathbb{V}\left(\hat{\beta} \right) =\sigma^2\left(X^{'}X \right)^{-1}=
    \sigma^2
    \begin{pmatrix}
    \frac{2(2n+1)}{n(n-1)} &
    -\frac{6}{n(n-1)} \\
    -\frac{6}{n(n-1)} &
    \frac{12}{n(n-1)(n+1)}
    \end{pmatrix}$$

    Asymptotiquement, on a alors

    $$
    \mathbb{V}\left(\hat{\beta} \right)  \underset{n\ \to\ + \infty}{\sim} \sigma^2
    \begin{pmatrix}
    \frac{4}{n} &
    -\frac{6}{n^2} \\
    -\frac{6}{n^2} &
    \frac{12}{n^3}
    \end{pmatrix}
    $$

    D'o√π
    $\mathbb{V}\left(\hat{\beta} \right)  \underset{n\ \to\ + \infty}{\longrightarrow} 0_{2,2}$.

b.  D'apr√®s la premi√®re question
    $\mathbb{V}\left(\hat{\beta} \right)=\textrm{EQM}\left(\hat{\beta} \right)$,
    donc il y a convergence en moyenne quadratique (i.e dans
    $\mathcal{L}^2$) de $\hat{\beta}$ vers $\beta$.
:::

:::

## Question 3

M√™me question lorsque les observations deviennent de plus en plus dense
dans un intervalle (pour simplifier : l‚Äôintervalle $[0, 1]$). On suppose
ainsi que $x_i = \frac{i}{n}$ pour tout $i \in [\![1,n]\!]$.

::: {.content-visible when-meta="is_answer_print_3"}

::: answer
Dans le cas o√π $x_i=\frac{i}{n}$ pour tout $i=1, \dots, n$, on a :

$$\begin{align*}
\overline{x} &= \frac{1}{n}\sum\limits_{i=1}^n x_i = \frac{1}{n}\sum\limits_{i=1}^n \frac{i}{n} =\frac{1}{n^2}\frac{n(n+1)}{2}=\frac{n+1}{2n} \\
\sum\limits_{i=1}^n x_i^2 &= \sum\limits_{i=1}^n \frac{i^2}{n^2} = \frac{(n+1)(2n+1)}{6n} \\ 
\sum\limits_{i=1}^n (x_i-\overline{x})^2 &=\sum\limits_{i=1}^n x_i^2-n\overline{x}^2=\frac{(n+1)(2n+1)}{6n}-\frac{(n+1)^2}{4n}=\frac{(n-1)(n+1)}{12n} 
\end{align*}$$

Ainsi,

$$
\mathbb{V}\left(\hat{\beta} \right) =\sigma^2\left(X^{'}X \right)^{-1}=
\sigma^2
\begin{pmatrix}
\frac{2(2n+1)}{n(n-1)} &
-\frac{6}{n-1} \\
-\frac{6}{n-1} &
\frac{12n}{(n-1)(n+1)}
\end{pmatrix}
$$

Asymptotiquement, on a alors

$$
\mathbb{V}\left(\hat{\beta} \right)   \underset{n\ \to\ + \infty}{\sim}
\sigma^2
\begin{pmatrix}
\frac{4}{n} &
-\frac{6}{n} \\
-\frac{6}{n} &
\frac{12}{n}
\end{pmatrix}
$$

$\textrm{EQM}\left(\hat{\beta} \right)=\mathbb{V}\left(\hat{\beta} \right) \underset{n\ \to\ + \infty}{\longrightarrow} 0_{2,2}$,
donc il y a convergence en moyenne quadratique (i.e dans
$\mathcal{L}^2$) de $\hat{\beta}$ vers $\beta$.
:::

:::

## Question 4

On se place ici dans un cas o√π les observations sont mal dispers√©es : on
suppose que $x_i = \frac{1}{i}$ pour tout $i = 1, \dots, n$. Ainsi les
observations se concentrent en 0. Qu‚Äôen est-il du comportement
asymptotique en moyenne quadratique de $\hat{\beta}_0$ et
$\hat{\beta}_1$ ?

::: {.content-visible when-meta="is_answer_print_4"}

::: answer
Dans le cas o√π $x_i=\frac{1}{i}$ pour tout $i=1, \dots, n$, on a :

$$
\begin{align*}
\overline{x} &= \frac{1}{n}\sum\limits_{i=1}^n x_i = \frac{1}{n}\sum\limits_{i=1}^n \frac{1}{i}  \\
\sum\limits_{i=1}^n x_i^2 &= \sum\limits_{i=1}^n \frac{1}{i^2} \\
\sum\limits_{i=1}^n (x_i-\overline{x})^2 &=\sum\limits_{i=1}^n x_i^2-n\overline{x}^2=\sum\limits_{i=1}^n \frac{1}{i^2}-\frac{1}{n}\left(\sum\limits_{i=1}^n \frac{1}{i} \right)^2
\end{align*}
$$

D'o√π

$$\mathbb{V}\left(\hat{\beta} \right)=\sigma^2(X'X)^{-1}=\frac{1}{\sum\limits_{i=1}^n \frac{1}{i^2}-\frac{1}{n}\left(\sum\limits_{i=1}^n \frac{1}{i} \right)^2}\begin{pmatrix}
\frac{1}{n}\sum\limits_{i=1}^n \frac{1}{i^2} & -\frac{1}{n}\sum\limits_{i=1}^n \frac{1}{i} \\
-\frac{1}{n}\sum\limits_{i=1}^n \frac{1}{i} & 1
\end{pmatrix}$$

Asymptotiquement, on a :

$$
\begin{align*}
\frac{1}{n}\sum\limits_{i=1}^n \frac{1}{i}  \ &\underset{n\ \to\ + \infty}{\sim} \frac{\ln(n)}{n} \\
\sum\limits_{i=1}^n \frac{1}{i^2}  &\underset{n\ \to\ + \infty}{\sim} \frac{\pi^2}{6}
\end{align*}
$$

Ainsi,

$$\mathbb{V}\left(\hat{\beta} \right)=\underset{n\ \to\ + \infty}{\sim} \sigma^2\begin{pmatrix}
\frac{1}{n} & -\frac{6\ln(n)}{n\pi^2} \\
-\frac{6\ln(n)}{n\pi^2}  & \frac{6}{\pi^2}
\end{pmatrix}$$

Dans ce $\hat{\beta}$ ne converge pas en moyenne quadratique vers
$\beta$.
:::

:::

## Question 5

Dans les exemples pr√©c√©dents, les $x_i$ √©taient d√©terministes. On
suppose ici que les $x_i$ sont al√©atoires, i.i.d, de carr√© int√©grable et
de variance non nulle. On suppose √©galement que les $x_i$ et les erreurs
de mod√©lisation $\epsilon_i$ sont ind√©pendants. Cette situation peut
√™tre vue comme l‚Äô√©quivalent al√©atoire des situations d√©terministes
trait√©es dans les questions 2 et 3 (selon que la loi des $x_i$ est
discr√®te ou continue).

a.  Exprimer $\hat{\beta}-\beta$ en fonction de la matrice $X$ et du
    vecteur $\epsilon$.

b.  En d√©duire que $\hat{\beta}$ converge presque s√ªrement vers $\beta$
    lorsque $n \to +\infty$.

::: {.content-visible when-meta="is_answer_print_5"}

::: answer
a.  Exprimons $\hat{\beta}-\beta$ en fonction de la matrice $X$ et du
    vecteur $\epsilon$. $$
    \hat{\beta}-\beta = (X'X)^{-1}X'Y-\beta = (X'X)^{-1}X'(X\beta+\epsilon)-\beta  = (X'X)^{-1}X'\epsilon
    $$

b.  On r√©injecte alors l'expression des matrices $X$ et $(X'X)^{-1}$
    dans le cas du mod√®le de r√©gression lin√©aire simple.
    $$\begin{align*}
    \hat{\beta}-\beta &= \frac{1}{\sum\limits_{i=1}^n (x_i-\overline{x})^2}
    \begin{pmatrix}
    \frac{1}{n}\sum\limits_{i=1}^n x_i^2 & -\overline{x} \\
    -\overline{x} & 1
    \end{pmatrix}
    \begin{pmatrix}
    1 & \dots & 1\\
    x_1 & \dots & x_n
    \end{pmatrix}
    \begin{pmatrix}
    \epsilon_1 \\
    \vdots  \\
    \epsilon_n \\
    \end{pmatrix} \\
    &= \frac{1}{\sum\limits_{i=1}^n (x_i-\overline{x})^2}
    \begin{pmatrix}
    \frac{1}{n}\sum\limits_{i=1}^n x_i^2 & -\overline{x} \\
    -\overline{x} & 1
    \end{pmatrix}
    \begin{pmatrix}
    \sum\limits_{i=1}^n \epsilon_i \\
    \sum\limits_{i=1}^n x_i\epsilon_i \\
    \end{pmatrix} \\
    &= \frac{1}{\sum\limits_{i=1}^n (x_i-\overline{x})^2}
    \begin{pmatrix}
    \overline{\epsilon}\sum\limits_{i=1}^n x_i^2-\overline{x}\sum\limits_{i=1}^n x_i\epsilon_i \\
    -\overline{x}\sum\limits_{i=1}^n \epsilon_i + \sum\limits_{i=1}^n x_i\epsilon_i
    \end{pmatrix} \\
    &=\frac{1}{\frac{1}{n}\sum\limits_{i=1}^n (x_i-\overline{x})^2}
    \begin{pmatrix}
    \overline{\epsilon}\frac{1}{n}\sum\limits_{i=1}^n x_i^2-\overline{x}\frac{1}{n}\sum\limits_{i=1}^n x_i\epsilon_i \\
    -\overline{x}\cdot \overline{\epsilon} + \frac{1}{n}\sum\limits_{i=1}^n x_i\epsilon_i
    \end{pmatrix}
    \end{align*}
    $$

    D'apr√®s la loi forte des grands nombres,

    -   comme les $x_i$ sont i.i.d, de carr√© int√©grable $$
        \frac{1}{n}\sum\limits_{i=1}^n (x_i-\overline{x})^2 \overset{ps}{\longrightarrow} \mathbb{V}(X)
        $$

    -   comme les $\epsilon_i$ sont i.i.d et dans $\mathcal{L}^2$ $$
        \frac{1}{n}\sum\limits_{i=1}^n \epsilon_i \overset{ps}{\longrightarrow} \mathbb{E}(\epsilon)=0
        $$

    -   comme les $x_i$ sont i.i.d et dans $\mathcal{L}^2$ $$
        \frac{1}{n}\sum\limits_{i=1}^n x_i^2 \overset{ps}{\longrightarrow} \mathbb{V}(X)+(\mathbb{E}(X))^2
        $$

    -   comme les $x_i$ sont i.i.d et dans $\mathcal{L}^1$ $$
        \frac{1}{n}\sum\limits_{i=1}^n x_i \overset{ps}{\longrightarrow} \mathbb{E}(X)
        $$

    -   comme les $x_i$ sont i.i.d et les $\epsilon_i$ sont i.i.d alors
        $x_i\epsilon_i$ sont i.i.d. de plus
        $\mathbb{E}(x_i\epsilon_i)=\mathbb{E}(x_i)\mathbb{E}(\epsilon_i)=0$
        car les $x_i$ et les $\epsilon_i$ sont ind√©pendants.

    $$
    \frac{1}{n}\sum\limits_{i=1}^n x_i\epsilon_i \overset{ps}{\longrightarrow} 0
    $$

    Ainsi, $\hat{\beta}$ converge presque s√ªrement vers $\beta$ lorsque
    $n \to +\infty$.
:::

:::
