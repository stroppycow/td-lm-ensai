---
title: "TD 1 - Exercice 5"
lang: fr
author: "Théo Leroy"
date: "3 septembre 2025"
format:
  live-html:
    code-background: true
    toc: true
    page-layout: full
webr:
  render-df: gt-interactive
fig-align: center
filters: 
  - diagram
  - custom-callout
editor: 
  mode: source
  markdown: 
    wrap: 72
diagram:
  engine:
    tikz:
      execpath: lualatex
      header-includes:
        - '\usepackage{tkz-tab}'
custom-callout:    
  answer:
    color: "#CCCCCC"
    appearance: "minimal"
    title: ""
---

{{< include ./../_extensions/r-wasm/live/_knitr.qmd >}}

On se place comme dans l’exercice précédent dans le cadre d’un modèle de
régression linéaire simple. On rappelle que la matrice de design $X$ et
la matrice $(X'X)^{-1}$ valent dans ce cas :

$$
X = \underbrace{\begin{pmatrix}
1 & x_1\\
\vdots & \vdots \\
1 & x_n
\end{pmatrix}}_{\in \mathcal{M}_{n,2}(\mathbb{R})}, \quad (X'X)^{-1}=\frac{1}{\sum\limits_{i=1}^n (x_i-\overline{x})^2}\begin{pmatrix}
\frac{1}{n}\sum\limits_{i=1}^n x_i^2 & -\overline{x} \\
-\overline{x} & 1
\end{pmatrix}
$$

On va examiner quelques exemples de design, c’est à dire de répartition
des valeurs de $x_1, \dots, x_n$, et vérifier la convergence (ou non)
des estimateurs par MCO des paramètres $\beta_0$ et $\beta_1$ dans
chaque cas.

## Question 1

Rappeler ce que vaut l’erreur quadratique moyenne de $\hat{\beta}$,
l’estimateur par MCO de
$\beta = \begin{pmatrix} \beta_0 & \beta_1 \end{pmatrix}'$.

::: {.content-hidden when-format="html"}
::: answer
L'erreur quadratique moyenne vaut

$$
\textrm{EQM}\left(\widehat{\beta} \right) = \mathbb{E}\left(\left[\widehat{\beta} -\beta \right]^2\right) =  \mathbb{V}\left(\widehat{\beta} \right) + \underbrace{\left(\mathbb{E}\left(\widehat{\beta} -\beta \right)\right)^2}_{=0 \textrm{ car } \widehat{\beta} \textrm{ est sans biais}}=\mathbb{V}\left(\widehat{\beta} \right) = \sigma^2\left(X^{'}X \right)^{-1}
$$
:::
:::

## Question 2

Dans ce premier exemple, on se place dans le cas où les observations ont
lieu de façon régulièrement espacées, et deviennent de plus en plus
nombreuses avec $n$. Quitte à renormaliser, on suppose ainsi que
$x_i = i$ pour tout $i \in [\![1,n]\!]$.

a.  Donner la limite de la matrice $\mathbb{V}(\hat{\beta})$ lorsque
    $n \to +\infty$.

b.  En déduire le comportement asymptotique en moyenne quadratique de
    $\hat{\beta}_0$ et $\hat{\beta}_1$.

::: {.content-hidden when-format="html"}
::: answer
a.  Dans le cas où $x_i=i$ pour tout $i=1, \dots, n$, on:

    $$\begin{align*}
    \overline{x} &= \frac{1}{n}\sum\limits_{i=1}^n x_i = \frac{1}{n}\sum\limits_{i=1}^n i =\frac{1}{n}\frac{n(n+1)}{2}=\frac{n+1}{2} \\
    \sum\limits_{i=1}^n x_i^2 &= \sum\limits_{i=1}^n i^2 = \frac{n(n+1)(2n+1)}{6} \\ 
    \sum\limits_{i=1}^n (x_i-\overline{x})^2&=\sum\limits_{i=1}^n x_i^2-n\overline{x}^2=\frac{n(n+1)(2n+1)}{6}-\frac{n(n+1)^2}{4}=\frac{n(n-1)(n+1)}{12} 
    \end{align*}$$

    Ainsi,

    $$\mathbb{V}\left(\hat{\beta} \right) =\sigma^2\left(X^{'}X \right)^{-1}=
    \sigma^2
    \begin{pmatrix}
    \frac{2(2n+1)}{n(n-1)} &
    -\frac{6}{n(n-1)} \\
    -\frac{6}{n(n-1)} &
    \frac{12}{n(n-1)(n+1)}
    \end{pmatrix}$$

    Asymptotiquement, on a alors

    $$\mathbb{V}\left(\hat{\beta} \right)  \underset{n\ \to\ + \infty}{\sim} \sigma^2
    \begin{pmatrix}
    \frac{4}{n} &
    -\frac{6}{n^2} \\
    -\frac{6}{n^2} &
    \frac{12}{n^3}
    \end{pmatrix}
    $$

    D'où
    $\mathbb{V}\left(\hat{\beta} \right)  \underset{n\ \to\ + \infty}{\longrightarrow} 0_{2,2}$.

b.  D'après la première question
    $\mathbb{V}\left(\hat{\beta} \right)=\textrm{EQM}\left(\hat{\beta} \right)$,
    donc il y a convergence en moyenne quadratique (i.e dans
    $\mathcal{L}^2$) de $\hat{\beta}$ vers $\beta$.
:::
:::

## Question 3

Même question lorsque les observations deviennent de plus en plus dense
dans un intervalle (pour simplifier : l’intervalle $[0, 1]$). On suppose
ainsi que $x_i = \frac{i}{n}$ pour tout $i \in [\![1,n]\!]$.

::: {.content-hidden when-format="html"}
::: answer
Dans le cas où $x_i=\frac{i}{n}$ pour tout $i=1, \dots, n$, on a :

$$\begin{align*}
\overline{x} &= \frac{1}{n}\sum\limits_{i=1}^n x_i = \frac{1}{n}\sum\limits_{i=1}^n \frac{i}{n} =\frac{1}{n^2}\frac{n(n+1)}{2}=\frac{n+1}{2n} \\
\sum\limits_{i=1}^n x_i^2 &= \sum\limits_{i=1}^n \frac{i^2}{n^2} = \frac{(n+1)(2n+1)}{6n} \\ 
\sum\limits_{i=1}^n (x_i-\overline{x})^2 &=\sum\limits_{i=1}^n x_i^2-n\overline{x}^2=\frac{(n+1)(2n+1)}{6n}-\frac{(n+1)^2}{4n}=\frac{(n-1)(n+1)}{12n} 
\end{align*}$$

Ainsi,

$$
\mathbb{V}\left(\hat{\beta} \right) =\sigma^2\left(X^{'}X \right)^{-1}=
\sigma^2
\begin{pmatrix}
\frac{2(2n+1)}{n(n-1)} &
-\frac{6}{n-1} \\
-\frac{6}{n-1} &
\frac{12n}{(n-1)(n+1)}
\end{pmatrix}
$$

Asymptotiquement, on a alors

$$
\mathbb{V}\left(\hat{\beta} \right)   \underset{n\ \to\ + \infty}{\sim}
\sigma^2
\begin{pmatrix}
\frac{4}{n} &
-\frac{6}{n} \\
-\frac{6}{n} &
\frac{12}{n}
\end{pmatrix}
$$

$\textrm{EQM}\left(\hat{\beta} \right)=\mathbb{V}\left(\hat{\beta} \right) \underset{n\ \to\ + \infty}{\longrightarrow} 0_{2,2}$,
donc il y a convergence en moyenne quadratique (i.e dans
$\mathcal{L}^2$) de $\hat{\beta}$ vers $\beta$.
:::
:::

## Question 4

On se place ici dans un cas où les observations sont mal dispersées : on
suppose que $x_i = \frac{1}{i}$ pour tout $i = 1, \dots, n$. Ainsi les
observations se concentrent en 0. Qu’en est-il du comportement
asymptotique en moyenne quadratique de $\hat{\beta}_0$ et
$\hat{\beta}_1$ ?

::: {.content-hidden when-format="html"}
::: answer
Dans le cas où $x_i=\frac{1}{i}$ pour tout $i=1, \dots, n$, on a :

$$
\begin{align*}
\overline{x} &= \frac{1}{n}\sum\limits_{i=1}^n x_i = \frac{1}{n}\sum\limits_{i=1}^n \frac{1}{i}  \\
\sum\limits_{i=1}^n x_i^2 &= \sum\limits_{i=1}^n \frac{1}{i^2} \\
\sum\limits_{i=1}^n (x_i-\overline{x})^2 &=\sum\limits_{i=1}^n x_i^2-n\overline{x}^2=\sum\limits_{i=1}^n \frac{1}{i^2}-\frac{1}{n}\left(\sum\limits_{i=1}^n \frac{1}{i} \right)^2
\end{align*}
$$

D'où

$$\mathbb{V}\left(\hat{\beta} \right)=\sigma^2(X'X)^{-1}=\frac{1}{\sum\limits_{i=1}^n \frac{1}{i^2}-\frac{1}{n}\left(\sum\limits_{i=1}^n \frac{1}{i} \right)^2}\begin{pmatrix}
\frac{1}{n}\sum\limits_{i=1}^n \frac{1}{i^2} & -\frac{1}{n}\sum\limits_{i=1}^n \frac{1}{i} \\
-\frac{1}{n}\sum\limits_{i=1}^n \frac{1}{i} & 1
\end{pmatrix}$$

Asymptotiquement, on a :

$$
\begin{align*}
\frac{1}{n}\sum\limits_{i=1}^n \frac{1}{i}  \ &\underset{n\ \to\ + \infty}{\sim} \frac{\ln(n)}{n} \\
\sum\limits_{i=1}^n \frac{1}{i^2}  &\underset{n\ \to\ + \infty}{\sim} \frac{\pi^2}{6}
\end{align*}
$$

Ainsi,

$$\mathbb{V}\left(\hat{\beta} \right)=\underset{n\ \to\ + \infty}{\sim} \sigma^2\begin{pmatrix}
\frac{1}{n} & -\frac{6\ln(n)}{n\pi^2} \\
-\frac{6\ln(n)}{n\pi^2}  & \frac{6}{\pi^2}
\end{pmatrix}$$

Dans ce $\hat{\beta}$ ne converge pas en moyenne quadratique vers
$\beta$.
:::
:::

## Question 5

Dans les exemples précédents, les $x_i$ étaient déterministes. On
suppose ici que les $x_i$ sont aléatoires, i.i.d, de carré intégrable et
de variance non nulle. On suppose également que les $x_i$ et les erreurs
de modélisation $\epsilon_i$ sont indépendants. Cette situation peut
être vue comme l’équivalent aléatoire des situations déterministes
traitées dans les questions 2 et 3 (selon que la loi des $x_i$ est
discrète ou continue).

a.  Exprimer $\hat{\beta}-\beta$ en fonction de la matrice $X$ et du
    vecteur $\epsilon$.

b.  En déduire que $\hat{\beta}$ converge presque sûrement vers $\beta$
    lorsque $n \to +\infty$.

::: {.content-hidden when-format="html"}
::: answer
a.  Exprimons $\hat{\beta}-\beta$ en fonction de la matrice $X$ et du
    vecteur $\epsilon$. $$
    \hat{\beta}-\beta = (X'X)^{-1}X'Y-\beta = (X'X)^{-1}X'(X\beta+\epsilon)-\beta  = (X'X)^{-1}X'\epsilon
    $$

b.  On réinjecte alors l'expression des matrices $X$ et $(X'X)^{-1}$
    dans le cas du modèle de régression linéaire simple.
    $$\begin{align*}
    \hat{\beta}-\beta &= \frac{1}{\sum\limits_{i=1}^n (x_i-\overline{x})^2}
    \begin{pmatrix}
    \frac{1}{n}\sum\limits_{i=1}^n x_i^2 & -\overline{x} \\
    -\overline{x} & 1
    \end{pmatrix}
    \begin{pmatrix}
    1 & \dots & 1\\
    x_1 & \dots & x_n
    \end{pmatrix}
    \begin{pmatrix}
    \epsilon_1 \\
    \vdots  \\
    \epsilon_n \\
    \end{pmatrix} \\
    &= \frac{1}{\sum\limits_{i=1}^n (x_i-\overline{x})^2}
    \begin{pmatrix}
    \frac{1}{n}\sum\limits_{i=1}^n x_i^2 & -\overline{x} \\
    -\overline{x} & 1
    \end{pmatrix}
    \begin{pmatrix}
    \sum\limits_{i=1}^n \epsilon_i \\
    \sum\limits_{i=1}^n x_i\epsilon_i \\
    \end{pmatrix} \\
    &= \frac{1}{\sum\limits_{i=1}^n (x_i-\overline{x})^2}
    \begin{pmatrix}
    \overline{\epsilon}\sum\limits_{i=1}^n x_i^2-\overline{x}\sum\limits_{i=1}^n x_i\epsilon_i \\
    -\overline{x}\sum\limits_{i=1}^n \epsilon_i + \sum\limits_{i=1}^n x_i\epsilon_i
    \end{pmatrix} \\
    &=\frac{1}{\frac{1}{n}\sum\limits_{i=1}^n (x_i-\overline{x})^2}
    \begin{pmatrix}
    \overline{\epsilon}\frac{1}{n}\sum\limits_{i=1}^n x_i^2-\overline{x}\frac{1}{n}\sum\limits_{i=1}^n x_i\epsilon_i \\
    -\overline{x}\cdot \overline{\epsilon} + \frac{1}{n}\sum\limits_{i=1}^n x_i\epsilon_i
    \end{pmatrix}
    \end{align*}
    $$

    D'après la loi forte des grands nombres,

    -   comme les $x_i$ sont i.i.d, de carré intégrable $$
        \frac{1}{n}\sum\limits_{i=1}^n (x_i-\overline{x})^2 \overset{ps}{\longrightarrow} \mathbb{V}(X)
        $$

    -   comme les $\epsilon_i$ sont i.i.d et dans $\mathcal{L}^2$ $$
        \frac{1}{n}\sum\limits_{i=1}^n \epsilon_i \overset{ps}{\longrightarrow} \mathbb{E}(\epsilon)=0
        $$

    -   comme les $x_i$ sont i.i.d et dans $\mathcal{L}^2$ $$
        \frac{1}{n}\sum\limits_{i=1}^n x_i^2 \overset{ps}{\longrightarrow} \mathbb{V}(X)+(\mathbb{E}(X))^2
        $$

    -   comme les $x_i$ sont i.i.d et dans $\mathcal{L}^1$ $$
        \frac{1}{n}\sum\limits_{i=1}^n x_i \overset{ps}{\longrightarrow} \mathbb{E}(X)
        $$

    -   comme les $x_i$ sont i.i.d et les $\epsilon_i$ sont i.i.d alors
        $x_i\epsilon_i$ sont i.i.d. de plus
        $\mathbb{E}(x_i\epsilon_i)=\mathbb{E}(x_i)\mathbb{E}(\epsilon_i)=0$
        car les $x_i$ et les $\epsilon_i$ sont indépendants.

    $$
    \frac{1}{n}\sum\limits_{i=1}^n x_i\epsilon_i \overset{ps}{\longrightarrow} 0
    $$

    Ainsi, $\hat{\beta}$ converge presque sûrement vers $\beta$ lorsque
    $n \to +\infty$.
:::
:::

::: {.content-hidden when-format="html"}
2+2
:::
