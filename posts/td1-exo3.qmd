---
title: "TD 1 - Exercice 3"
lang: fr
author: "ThÃ©o Leroy"
date: "3 septembre 2025"
params:
  question_courante: 10
format:
  live-html:
    code-background: true
    toc: true
    page-layout: full
webr:
  render-df: gt-interactive
fig-align: center
filters: 
  - custom-callout
editor: 
  mode: source
  markdown: 
    wrap: 72
custom-callout:    
  answer:
    color: "#CCCCCC"
    icon: true
    icon-symbol: "ğŸ“"
    appearance: "default"
    title: "Correction"
---

{{< include ./../_extensions/conditionnal.qmd >}}
{{< include ./../_extensions/r-wasm/live/_knitr.qmd >}}

On considÃ¨re le modÃ¨le de rÃ©gression linÃ©aire simple oÃ¹ lâ€™on observe $n$
rÃ©alisations $(x_i, y_i)_{i \in [\![1,n]\!]}$; liÃ©es par la relation
$y_i = \beta_0 + \beta_1x_i + \epsilon_i$; $i = 1, \ \dots, \ n$. On
suppose que les $x_i$ sont dÃ©terministes et que les variables
$\epsilon_i$ sont centrÃ©es, de variance $\sigma^2$ et non-corrÃ©lÃ©es
entre elles.

## Question 1

Ã‰crire le modÃ¨le sous forme matricielle

::: {.content-visible when-meta="is_answer_print_1"}

::: answer
Le modÃ¨le s'Ã©crit matricellement $Y=X\beta+\epsilon$ avec

$$
Y = \underbrace{\begin{pmatrix}
y_1\\
\vdots \\
y_n
\end{pmatrix}}_{\in \mathcal{M}_{n,1}(\mathbb{R})} \qquad
X = \underbrace{\begin{pmatrix}
1 & x_1\\
\vdots & \vdots \\
1 & x_n
\end{pmatrix}}_{\in \mathcal{M}_{n,2}(\mathbb{R})} \qquad
\beta = \underbrace{\begin{pmatrix}
\beta_0 \\
\beta_1
\end{pmatrix}}_{\in \mathcal{M}_{2,1}(\mathbb{R})} \qquad
\epsilon =\underbrace{\begin{pmatrix}
\epsilon_1\\
\vdots \\
\epsilon_n
\end{pmatrix}}_{\in \mathcal{M}_{n,1}(\mathbb{R})} 
$$

avec les hypothÃ¨ses
$\mathbb{E}(\epsilon) =0_{n,1} \ \textrm{et} \ \mathbb{V}(\epsilon)=\sigma^2I_n$.
:::

:::

## Question 2

De quel problÃ¨me de minimisation lâ€™estimateur des moindres carrÃ©s
$\hat{\beta}= (\hat{\beta}_0, \hat{\beta}_1)$ est-il la solution ?

::: {.content-visible when-meta="is_answer_print_2"}

::: answer
L'estimateur des moindres carrÃ©s ordinaires est solution du problÃ¨me :
$$
\underset{(\beta_0, \beta_1) \in \mathbb{R}^2}{{\textrm{argmin}}} \ \sum\limits_{i=1}^n\left(y_i-\beta_0-\beta_1x_i\right)^2
$$
:::

:::

## Question 3

On peut trouver $\hat{\beta}$ en annulant le gradient de la fonction Ã 
minimiser prÃ©cÃ©dente. Cela a dÃ©jÃ  Ã©tÃ© fait et les solutions sont Ã 
connaitre par coeur : que valent-elles ?

::: {.content-visible when-meta="is_answer_print_3"}

::: answer
On a (*cf slide 24*)

$$\hat{\beta}_1=\frac{\textrm{cov}(x,y)}{\textrm{var}(x)}=\frac{\frac{1}{n}\sum\limits_{i=1}^n\left(x_i-\overline{x}\right)\left(y_i-\overline{y}\right)}{\frac{1}{n}\sum\limits_{i=1}^n\left(x_i-\overline{x}\right)^2} \qquad \hat{\beta}_0= \overline{y}-\hat{\beta}_1\overline{x}$$

::: {.callout-note collapse="true"}
### Preuve

On considÃ¨re la fonction

$$
S(\beta_0,\beta_1) \;=\; \sum_{i=1}^n \left( y_i - \beta_0 - \beta_1 x_i \right)^2,
\qquad S:\mathbb{R}^2 \to \mathbb{R}.
$$

**Calcul du gradient**

On calcule les dÃ©rivÃ©es partielles :

$$
\begin{align*}
\frac{\partial S}{\partial \beta_0}
&= -2 \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)
= 2\Big( n\beta_0 + \beta_1 \sum_{i=1}^n x_i - \sum_{i=1}^n y_i \Big)
\\
\frac{\partial S}{\partial \beta_1}
&= -2 \sum_{i=1}^n x_i (y_i - \beta_0 - \beta_1 x_i)
= 2\Big( \beta_0 \sum_{i=1}^n x_i + \beta_1 \sum_{i=1}^n x_i^2 - \sum_{i=1}^n x_i y_i \Big)
\end{align*}
$$

Ainsi

$$
\nabla S(\beta_0,\beta_1)
= 2\begin{pmatrix}
n\beta_0+\beta_1 \sum\limits_{i=1}^n x_i - \sum\limits_{i=1}^n  y_i \\
\beta_0 \sum\limits_{i=1}^n  x_i + \beta_1 \sum\limits_{i=1}^n  x_i^2 - \sum\limits_{i=1}^n  x_i y_i
\end{pmatrix}
$$

**Ã‰quations normales**

Les conditions du premier ordre donnent :

$$
\begin{align*}
\begin{cases}
n\beta_0 + \beta_1 \sum\limits_{i=1}^n x_i - \sum\limits_{i=1}^n y_i =0\\
\beta_0 \sum\limits_{i=1}^n x_i + \beta_1 \sum\limits_{i=1}^n x_i^2 - \sum\limits_{i=1}^n x_i y_i=0
\end{cases}
&\Longleftrightarrow
\begin{cases}
\beta_0 = \overline{y}-\beta_1\overline{x}\\
n\overline{x}\beta_0+ \beta_1 \sum\limits_{i=1}^n x_i^2 = \sum\limits_{i=1}^n x_i y_i
\end{cases} \\
&\Longleftrightarrow 
\begin{cases}
\beta_0 = \overline{y}-\beta_1\overline{x}\\
n\overline{x}\left(\overline{y}-\beta_1\overline{x}\right)+ \beta_1 \sum\limits_{i=1}^n x_i^2 = \sum\limits_{i=1}^n x_i y_i
\end{cases} \\
&\Longleftrightarrow 
\begin{cases}
\beta_0 = \overline{y}-\beta_1\overline{x}\\
\left(\sum\limits_{i=1}^n x_i^2-n\overline{x}^2\right)\beta_1=\sum\limits_{i=1}^n x_i y_i-n\overline{x}\overline{y}
\end{cases} \\
&\Longleftrightarrow 
\begin{cases}
\beta_0 = \overline{y}-\beta_1\overline{x}\\
\textrm{var}(x)\beta_1=\textrm{cov}(x,y)
\end{cases} \\
\end{align*}
$$

**Hessienne et convexitÃ©**

La matrice Hessienne $H_S$ de $S$ est

$$
H_S(\beta_0,\beta_1)
= \begin{pmatrix}
\frac{\partial^2 S}{\partial \beta_0^2} & \frac{\partial^2 S}{\partial \beta_0\partial \beta_1} \\
\frac{\partial^2 S}{\partial \beta_1\partial \beta_0} & \frac{\partial^2 S}{\partial \beta_1^2}
\end{pmatrix} = 
2\begin{pmatrix}
n & \sum\limits_{i=1}^n x_i \\
\sum\limits_{i=1}^n x_i & \sum\limits_{i=1}^n x_i^2
\end{pmatrix}
$$

Montrons que la matrice $H_S$ est semi dÃ©finie positive et donc que la
fonction $S$ est convexe.

Soit $v = (v_1, v_2)\in\mathbb{R}^2$,

$$
\begin{align*}
v' H_S v =2\begin{pmatrix}
v_1 & v_2
\end{pmatrix}
\begin{pmatrix}
n & \sum\limits_{i=1}^n x_i \\
\sum\limits_{i=1}^n x_i & \sum\limits_{i=1}^n x_i^2
\end{pmatrix}
\begin{pmatrix}
v_1 \\
v_2
\end{pmatrix}&=
2\begin{pmatrix}
v_1 & v_2
\end{pmatrix}
\begin{pmatrix}
v_1n +v_2 \sum\limits_{i=1}^n x_i \\
v_1 \sum\limits_{i=1}^n x_i +v_2 \sum\limits_{i=1}^n x_i ^2
\end{pmatrix} \\
&=2\left(nv_1^2+2v_1v_2\sum\limits_{i=1}^n x_i +v_2^2 \sum\limits_{i=1}^n x_i^2 \right) \\
&=\underbrace{2\sum\limits_{i=1}^n(v_1+x_iv_2)^2}_{\geq 0}
\end{align*}
$$

Donc $H_S$ est semi-dÃ©finie positive et $S$ est convexe.

**UnicitÃ© et conclusion**

Montrons que la matrice $H_S$ est dÃ©finie positive et donc que la
fonction $S$ est strictement convexe si et seulement si les $x_i$ ne
sont pas tous Ã©gaux.

D'aprÃ¨s les rÃ©sultats obtenus dans le paragraphe prÃ©cÃ©dent,
$v' H_S v = 0$ si et seulement si $v_0 + v_1 x_i = 0$ pour tout
$i \in [\![1,\dots,n]\!]$.

On en dÃ©duit

$$
\forall i \in [\![1,\dots,n]\!], v_1x_i=-v_0 \textrm{ et } \forall i \in [\![1,\dots,n]\!], v_1^2x_i^2=v_0^2
$$

En sommant ces Ã©galitÃ©s, on a :

$$
\sum\limits_{i=1}^n v_1x_i=-nv_0 \quad \sum\limits_{i=1}^n v_1^2x_i^2=nv_0^2
$$

D'oÃ¹,

$$v_1^2\underbrace{\left(\frac{1}{n}\sum\limits_{i=1}^n x_i^2-\left(\frac{1}{n}\sum\limits_{i=1}^n x_i \right)^2\right)}_{\textrm{var}(x)}=0$$

Ainsi,

-   Si $\textrm{var}(x) \neq 0$, c'est Ã  dire si et seulement si tous
    les $x_i$ sont ne sont pas tous Ã©gaux alors
    $v' H_S v = 0 \Leftrightarrow v= (0, 0)$ donc $H_S$ est une matrice
    dÃ©finie positive et $S$ est strictement convexe. La fonction $S$
    admet un unique minimum global atteint en

$$
\hat{\beta}_1=\frac{\textrm{cov}(x,y)}{\textrm{var}(x)} \qquad
\hat{\beta}_0 = \overline{y}-\hat{\beta}_1\overline{x} 
$$

-   Si $\textrm{var}(x)=0$, c'est Ã  dire si et seulement si tous les
    $x_i=\overline{x}$ alors $v' H_S v = 0 \nLeftrightarrow v= (0, 0)$
    donc $H_S$ n'est pas une matrice dÃ©finie positive et $S$ est convexe
    mais pas strictement. La fonction $S$ admet un minimum global mais
    il existe une infinitÃ© de couples $(\hat{\beta}_0,\hat{\beta}_1)$
    minimisant $S$ (i.e. la valeur minimale est unique, mais les
    minimisateurs ne le sont pas). Ils vÃ©rifient la relation :
    $$\hat{\beta}_0 = \overline{y}-\hat{\beta}_1\overline{x}$$
:::

:::

:::

## Question 4

Retrouver $\hat{\beta}$ en utilisant la formule gÃ©nÃ©rale
$\hat{\beta} = (X'X)^{-1}X'Y$:

::: {.content-visible when-meta="is_answer_print_4"}

::: answer
On calcule $\hat{\beta}$ Ã  partir de la formule gÃ©nÃ©rale Ã©tape par Ã©tape
:

$$
\begin{align*}
X &= \underbrace{\begin{pmatrix}
1 & x_1\\
\vdots & \vdots \\
1 & x_n
\end{pmatrix}}_{\in \mathcal{M}_{n,2}(\mathbb{R})} \\
X' &= \underbrace{\begin{pmatrix}
1 & \dots & 1\\
x_1 & \dots & x_n
\end{pmatrix}}_{\in \mathcal{M}_{2,n}(\mathbb{R})} \\
X'X &= \begin{pmatrix}
1 & \dots & 1\\
x_1 & \dots & x_n
\end{pmatrix}\begin{pmatrix}
1 & x_1 \\
\vdots & \vdots \\
1 & x_n
\end{pmatrix}=\underbrace{\begin{pmatrix}
n & \sum\limits_{i=1}^n x_i \\
\sum\limits_{i=1}^n x_i & \sum\limits_{i=1}^n x_i^2
\end{pmatrix}}_{\in \mathcal{M}_{2,2}(\mathbb{R})}
\end{align*}
$$

Si $X'X$ est inversible i.e. si $\mathrm{det}(X'X) \neq 0
\Longleftrightarrow \textrm{var}(x) \neq 0 \Longleftrightarrow$ tous les
$x_i$ ne sont pas Ã©gaux, on a :

$$
\begin{align*}
(X'X)^{-1}&=\frac{1}{\det(X'X)}\left(\textrm{Com(X'X)}\right)'=\frac{1}{n^2\textrm{var}(x)}\begin{pmatrix}
\sum\limits_{i=1}^n x_i^2 & -\sum\limits_{i=1}^n x_i \\
-\sum\limits_{i=1}^n x_i & n
\end{pmatrix} = \underbrace{\frac{1}{n\textrm{var}(x)}\begin{pmatrix}
\overline{x^2} & -\overline{x} \\
-\overline{x} & 1
\end{pmatrix}}_{\in \mathcal{M}_{2,2}(\mathbb{R})}\\
X'Y&=\begin{pmatrix}
1 & \dots & 1\\
x_1 & \dots & x_n
\end{pmatrix}\begin{pmatrix}
y_1 \\
\vdots \\
y_n
\end{pmatrix}=
\begin{pmatrix}
\sum\limits_{i=n}^n y_i \\
\sum\limits_{i=n}^n x_iy_i
\end{pmatrix}
=\underbrace{n\begin{pmatrix}
\overline{y} \\
\overline{xy}
\end{pmatrix}}_{\in \mathcal{M}_{2,1}(\mathbb{R})}
\end{align*}
$$

Ainsi,

$$
\begin{align*}
\hat{\beta} &= (X'X)^{-1}X'Y=\frac{n}{n\textrm{var}(x)}\begin{pmatrix}
\overline{x^2} & -\overline{x} \\
-\overline{x} & 1
\end{pmatrix}\begin{pmatrix}
\overline{y} \\
\overline{xy}
\end{pmatrix} \\
&=\frac{1}{\textrm{var}(x)}\begin{pmatrix}
\overline{x^2}\cdot\overline{y}-\overline{x}\cdot\overline{xy} \\
\overline{xy}-\overline{x}\cdot\overline{y} \\
\end{pmatrix} \\
&=\frac{1}{\textrm{var}(x)}\begin{pmatrix}
\overline{x^2}\cdot\overline{y}-\overline{y}\cdot\overline{x}^2-\overline{x}\cdot\overline{xy}+\overline{y}\cdot\overline{x}^2 \\
\overline{xy}-\overline{x}\cdot\overline{y} \\
\end{pmatrix} \\
&=\frac{1}{\textrm{var}(x)}\begin{pmatrix}
\overline{y}\textrm{var}(x)-\overline{x}\textrm{cov}(x,y) \\
\overline{xy}-\overline{x}\cdot\overline{y} \\
\end{pmatrix} \\
&=\begin{pmatrix}
\overline{y}-\overline{x}\frac{\textrm{cov}(x,y)}{\textrm{var}(x)}
 \\
\frac{\textrm{cov}(x,y)}{\textrm{var}(x)}
\end{pmatrix}
\end{align*}
$$
:::

:::

## Question 5

Justifier pourquoi la droite de rÃ©gression passe nÃ©cessairement par le
point $(\overline{x}; \overline{y})$.

::: {.content-visible when-meta="is_answer_print_5"}

::: answer
D'aprÃ¨s la question 3, on a :
$\hat{\beta}_0 = \overline{y}-\hat{\beta}_1\overline{x}$
:::

:::

## Question 6

On souhaite prÃ©dire la valeur $y_o$ associÃ©e Ã  la valeur $x_o$ dâ€™un
nouvel individu, en supposant que ce dernier suit exactement le mÃªme
modÃ¨le que les $n$ individus prÃ©cÃ©dents. Que vaut la prÃ©vision
$\hat{y}_o$ de $y_o$ ?

::: {.content-visible when-meta="is_answer_print_6"}

::: answer
On a $\hat{y}_o = \hat{\beta}_0+\hat{\beta}_1x_o$
:::

:::

## Question 7

Montrer que lâ€™espÃ©rance de lâ€™erreur de prÃ©vision $\hat{y}_o$ de $y_o$
est nulle.

::: {.content-visible when-meta="is_answer_print_7"}

::: answer
On calcule l'espÃ©rance de l'erreur de prÃ©vision

$$
\begin{align*}
\mathbb{E}(y_o-\hat{y}_o)
&=\mathbb{E}(y_o) - \mathbb{E}(\hat{y}_o)\ \textrm{par linÃ©aritÃ© de l'espÃ©rance} \\
&=\mathbb{E}(\beta_0+\beta_1x_o+\epsilon_o)-\mathbb{E}(\hat{\beta}_0+\hat{\beta}_1x_o) \\
&=\mathbb{E}(\beta_0)+x_o\mathbb{E}(\beta_1)+\mathbb{E}(\epsilon_o)-\mathbb{E}(\hat{\beta}_0)-x_o\mathbb{E}(\hat{\beta}_1) \ \textrm{par linÃ©aritÃ© de l'espÃ©rance}
\end{align*}
$$

Or

-   $\mathbb{E}(\hat{\beta}_0)=\beta_0$ et
    $\mathbb{E}(\hat{\beta}_1)=\beta_1$ car l'estimateur des moindres
    carrÃ©s ordinaires est sans biais ;
-   $\mathbb{E}(\beta_0)=\beta_0$ et $\mathbb{E}(\beta_1)=\beta_1$ car
    $\beta_0$ et $\beta_1$ sont dÃ©terministes ;
-   $\mathbb{E}(\epsilon_o)=0$ par hypothÃ¨se du modÃ¨le.

Ainsi,
$$
\mathbb{E}(y_o-\hat{y}_o)=0
$$

::: {.callout-note collapse="true"}
### Rappel de la preuve : l'estimateur MCO d'un modÃ¨le de rÃ©gression linÃ©aire simple est sans biais

On reprend le rÃ©sultat de la question 3

$$
\widehat{\beta}_1 = \frac{\sum_\limits{i=1}^n (x_i-\overline{x})(y_i-\overline{y})}{\sum\limits_{i=1}^n (x_i-\overline{x})^2},\qquad
\widehat{\beta}_0 = \overline{y}-\widehat{\beta}_1\,\overline{x}.
$$

Remplacer $y_i=\beta_0+\beta_1 x_i+\epsilon_i$ dans le numÃ©rateur de
$\widehat{\beta}_1$ donne

$$
\sum_{i=1}^n (x_i-\overline{x})(y_i-\overline{y})
=\sum_{i=1}^n (x_i-\overline{x})(\beta_1(x_i-\overline{x})+\epsilon_i)
=\beta_1\sum_{i=1}^n (x_i-\overline{x})^2 + \sum_{i=1}^n (x_i-\overline{x})\epsilon_i.
$$

En prenant l'espÃ©rance (comme les $x_i$ sont dÃ©terministes et
$\mathbb{E}(\epsilon_i)=0$), on obtient

$$
\mathbb{E}\left(\sum_{i=1}^n (x_i-\overline{x})(y_i-\overline{y})\right)
=\beta_1\sum_{i=1}^n (x_i-\overline{x})^2 + \sum_{i=1}^n (x_i-\overline{x})\mathbb{E}(\epsilon_i)
=\beta_1\sum_{i=1}^n (x_i-\overline{x})^2
$$

Donc

$$
\mathbb{E}(\widehat{\beta}_1)=\beta_1
$$

Pour $\widehat{\beta}_0$, on utilise
$\widehat{\beta}_0=\overline{y}-\widehat{\beta}_1\,\overline{x}$ et la
linÃ©aritÃ© de l'espÃ©rance :

$$
\mathbb{E}(\hat{\beta}_0)=\mathbb{E}(\overline{y})-\mathbb{E}(\widehat{\beta}_1)\overline{x}
= (\beta_0+\beta_1\overline{x}) - \beta_1\overline{x} = \beta_0
$$

Ainsi, sous les hypothÃ¨ses usuelles,
$\hat{\beta}=\left(\hat{\beta}_0,\hat{\beta}_1\right)=(0,0)$ est sans
biais.
:::

::: {.callout-note collapse="true"}
### Rappel de la preuve : l'estimateur MCO d'un modÃ¨le de rÃ©gression linÃ©aire multiple est sans biais

L'estimateur des moindres carrÃ©s ordinaires s'Ã©crit

$$
\hat{\beta}=\left(X'X\right)^{-1}X'Y
$$

En substituant $\mathbf{Y}=X\beta+\epsilon$ on obtient

$$
\widehat{\beta}=(X^\top X)^{-1}X^\top (X\beta+\varepsilon) = \beta + (X^\top X)^{-1}X^\top\varepsilon.
$$

En prenant l'espÃ©rance (conditionnelle sur $X$ fixÃ©, ou sous l'hypothÃ¨se
$\mathbb{E}(\epsilon\mid X)=0$), on obtient

$$
\mathbb{E}(\hat{\beta}) = \beta + (X' X)^{-1}X'\mathbb{E}(\epsilon) = \beta.
$$

Donc $\hat{\beta}$ est sans biais.
:::

:::

:::

## Question 8

Pour un modÃ¨le de rÃ©gression linÃ©aire gÃ©nÃ©rale, la variance de lâ€™erreur
de prÃ©vision associÃ©e Ã  un nouveau vecteur de rÃ©gresseur $x$, de
dimension $p$, vaut (cf cours) : $\sigma^2(x'(X'X)^{-1}x+1)$. Montrer
quâ€™ici cette variance peut se rÃ©crire :

$$
\sigma^2\left(1+\frac{1}{n}+\frac{(x_o-\overline{x})^2}{\sum\limits_{i=1}^n (x_i-\overline{x})^2} \right)
$$

::: {.content-visible when-meta="is_answer_print_8"}

::: answer
On applique la formule du cours avec
$x= \begin{pmatrix} 1 & x_o\end{pmatrix}'$ et la matrice $(X'X)^{-1}$
dans le cas du modÃ¨le de regression linÃ©aire simple qu'on a pu calculer
Ã  la question 4.

On a alors :

$$
\begin{align*}
\sigma^2(x'(X'X)^{-1}x+1)&=\sigma^2\left(\frac{1}{n\textrm{var}(x)}\begin{pmatrix}
1 & x_o
\end{pmatrix}
\begin{pmatrix}
\overline{x^2} & -\overline{x} \\
-\overline{x} & 1
\end{pmatrix}
\begin{pmatrix}
1 \\
x_o
\end{pmatrix} +1 \right) \\
&=\sigma^2\left(\frac{1}{n\textrm{var}(x)}\begin{pmatrix}
1 & x_o
\end{pmatrix}
\begin{pmatrix}
\overline{x^2}- x_o\overline{x} \\
-\overline{x} +x_o
\end{pmatrix} +1 \right) \\
&=\sigma^2\left(\frac{1}{n\textrm{var}(x)}\left(\overline{x^2}- 2x_o\overline{x}+x_o^2  \right) +1 \right) \\
&=\sigma^2\left(\frac{1}{n\textrm{var}(x)}\left(\textrm{var}(x)+\overline{x}^2- 2x_o\overline{x}+x_o^2  \right) +1 \right) \\
&=\sigma^2\left(1+\frac{1}{n}+\frac{(x_o-\overline{x})^2}{\sum\limits_{i=1}^n (x_i-\overline{x})^2} \right)
\end{align*}
$$
:::

:::

## Question 9

Discuter de la qualitÃ© de la prÃ©vision selon que $x_o$ est proche ou non
de la moyenne empirique $x_n$.

::: {.content-visible when-meta="is_answer_print_9"}

::: answer
D'aprÃ¨s la question prÃ©cÃ©dente, la variance croÃ®t avec
$(x_o - \overline{x})^2$ ; la prÃ©vision est dâ€™autant plus prÃ©cise
lorsque $x_o$ est proche de la moyenne $\overline{x}$.
:::

:::

## Question 10

Quâ€™en est-il si $n$ est grand ?

::: {.content-visible when-meta="is_answer_print_10"}

::: answer
Il n'est pas possible de rÃ©pondre en toute gÃ©nÃ©ralitÃ© car cela dÃ©pend du
comportement asymptotique de la suite des valeurs Ã©chantillonnÃ©es
$(x_i)_{i \in \mathbb{N}}$

On supposer une condition de rÃ©gularitÃ©

$$
\begin{align*}
\frac{1}{n}\sum\limits_{i=1}^n x_i \underset{n \to +\infty}{\longrightarrow} m \quad \textrm{et} \quad 
\frac{1}{n}\sum\limits_{i=1}^n (x_i-\overline{x})^2 \underset{n \to +\infty}{\longrightarrow} \rho > 0
\end{align*}
$$

Cela signifie que les $x_i$ ne deviennent pas tous Ã©gaux (variance non
dÃ©gÃ©nÃ©rÃ©e), et que leur dispersion ne Â« sâ€™effondre Â» ou au contraire Â«
n'explose Â» pas quand $n$ augmente. Cette hypothÃ¨se un peu un Ã©quivalent
dÃ©terministe au fait de supposer que $X$ soit une variable alÃ©atoire de
carrÃ© intÃ©grable.

On aurait alors

$$
\frac{(x_o-\overline{x})^2}{n} \underset{n \to +\infty}{\longrightarrow} 0
$$

puis

$$
\frac{(x_o-\overline{x})^2}{\sum\limits_{i=1}^n (x_i-\overline{x})^2}=\frac{(x_o-\overline{x})^2}{n} \times\frac{1}{\frac{1}{n}\sum\limits_{i=1}^n (x_i-\overline{x})^2 }\underset{n \to +\infty}{\longrightarrow}0
$$
D'oÃ¹

$$
\mathbb{V}(\hat{y}_o) \underset{n \to +\infty}{\longrightarrow} \sigma^2
$$

Ainsi lorsque $n$ est grand, seul le terme $\sigma^2$ reste dominant
dans la variance de prÃ©vision. La position de $x_o$ par rapport Ã 
$\overline{x}$ devient peu importante en comparaison.
:::

:::

