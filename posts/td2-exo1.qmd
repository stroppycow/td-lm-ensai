---
title: "TD 2 - Exercice 1"
lang: fr
author: "Théo Leroy"
date: "8 octbre 2024"
format:
  live-html:
    code-background: true
    toc: true
    page-layout: full
webr:
  render-df: gt-interactive
  resources:
    - ./../data/eucalyptus.txt
  packages:
      - dplyr
      - readr
      - ggplot2
      - cowplot
fig-align: center
editor: 
  markdown: 
    wrap: 72
---

![](/static/img/eucalyptus.jpg){fig-align="center"}

{{< include ./../_extensions/r-wasm/live/_knitr.qmd >}}

## Question 1

On charge les données du fichier `eucalyptus.txt` en R sous forme de
`data.frame`.

::: {.panel-tabset group="language"}
### Base R

```{webr}
#| envir: baser
#| autorun: true
data_eucalyptus <- read.csv(
  file = "data/eucalyptus.txt",
  header = TRUE,
  sep = " ",
  quote = '"'
)
data_eucalyptus
```

### Tidyverse

```{webr}
#| envir: tdv
#| autorun: true
data_eucalyptus <- readr::read_delim(
  file = "data/eucalyptus.txt",
  col_types = "_nnic",
  col_names = c("ht", "circ", "bloc", "clone"),
  skip = 1,
  delim = " ",
  quote = '"'
)
data_eucalyptus
```
:::

Le fichier chargé contient bien 1 429 observations avec les variables
`ht` pour la hauteur (exprimée en mètre) et `circ` pour la circonférence
des eucalyptus.

On représente ces données dans le plan.

::: {.panel-tabset group="language"}
### Base R

```{webr}
#| envir: baser
#| autorun: true
#| fig-align: center
plot(
  x = data_eucalyptus$circ,
  y = data_eucalyptus$ht,
  type ="p",
  xlab = "Circonférence",
  ylab = "Hauteur",
  pch = 3
)
grid()
```

### Tidyverse

```{webr}
#| envir: tdv
#| autorun: true
#| fig-align: center
ggplot2::ggplot(
  data = data_eucalyptus,
  mapping = aes(x = circ, y = ht)
) +
  ggplot2::labs(x = "Circonférence", y = "Hauteur") + 
  ggplot2::geom_point()
```
:::

## Question 2

On effectue la régression $y = \beta_1+\beta_2x+\varepsilon$ à l'aide de
la fonction `lm` disponible nativement dans
{{< iconify logos:r-lang >}}. On cherche ainsi une estimation de
$\beta_1$ et $\beta_2$ à l'aide de la méthode des moindres carrés
ordinaires (MCO).

::: {.panel-tabset group="language"}
### Base R

```{webr}
#| envir: baser
#| autorun: true
fit_lm_eucalyptus <- lm(ht ~ circ, data = data_eucalyptus) 
summary(fit_lm_eucalyptus)
```

### Tidyverse

```{webr}
#| envir: tdv
#| autorun: true
fit_lm_eucalyptus <- lm(ht ~ circ, data = data_eucalyptus) 
summary(fit_lm_eucalyptus)
```
:::

Commentons la sortie détaillée donnée par la fonction générique
`summary`.

-   La première partie du tableau <tt>Residuals</tt> donne quelques
    statistiques descriptives sur la distribution des résidus de la
    régression linéaire Pour rappel, le résidu $\widehat{\varepsilon}_i$
    de l'observation $i$ est la différence entre la variable à expliquer
    $Y_i$ et la valeur ajustée de $Y_i$ par le modèle c'est à dire :

    $$
    \widehat{\varepsilon}_i = Y_i- \underbrace{(\widehat{\beta}_1 + \widehat{\beta}_2X_i)}_{\widehat{Y}_i}
    $$

-   La seconde partie notée <tt>Coefficients</tt> est présentée sous
    forme de tableau. Elle nous renseigne sur l'estimation des
    coefficients et la significativité de ces derniers sous l'hypothèse
    de gaussianité des erreurs. Ce tableau comporte autant de lignes que
    de coefficient à estimer dans le modèle de régression linéaire (ce
    nombre est noté $p$ dans le cours). Ici, il y a deux lignes car deux
    coefficients interviennent dans notre modélisation, la première
    ligne correspond à l'estimation de $\beta_1$ pour l'ordonnée à
    l'origine <tt>Intercept</tt>, et la deuxième sur l'estimation de
    $\beta_2$ , l'effet de la circonférence <tt>circ</tt> sur la hauteur
    de l'arbre.

    -   La première colonne (colonne <tt>Estimate</tt>) contient les
        estimations des paramètres i.e. les estimations
        $\widehat{\beta}_1 \approx 9,03$ et
        $\widehat{\beta}_2 \approx 0,26$.

        L'estimation du coefficient estimété $\widehat{\beta}_2$
        interprète de la manière suivante : "Lorsque la circonférence
        d'un arbre augmente d'un centimètre sa hauteur gagne 26
        centimètres."

    -   La seconde colonne ( <tt>Std. Error</tt>) contient les
        écarts-types estimés des estimateurs $\widehat{\beta}_1$ et
        $\widehat{\beta}_2$ . Ils sont donnés par

        $$
        \forall\ j \in \{1, \dots, p\}, \  \widehat{\sigma}_{\widehat{\beta}_j} = \widehat{\sigma}\sqrt{(X'X)^{-1}_{j,j}} \ \textrm{avec} \ \widehat{\sigma} = \sqrt{\frac{1}{n-p}\sum\limits_{i=1}^n \widehat{\varepsilon}_i^2}
        $$

    -   Dans la troisième colonne (<tt>t. value</tt>) figure la valeur
        observée de la statistique de test de Student d'hypothèse
        $H_0 : \beta_ j = 0$ contre $H_1 : \beta_j \neq 0$.

        ::: callout-important
        Ce test est valable uniquement si sous l'hypothèse de normalité
        et indépendance des erreurs, c'est à dire si on suppose que :

        $$
        \varepsilon  = \begin{pmatrix} 
        \varepsilon_1 \\
        \vdots \\
        \varepsilon_n
        \end{pmatrix} \sim \mathcal{N}\left(0, \sigma^2 I_n \right)
        $$
        :::

        La statistique de test $T_j$ est donnée par

        $$
        T_j = \frac{{\widehat{\beta}}_j}{\widehat{\sigma}_{\widehat{\beta}_j}}
        $$

        Sous l'hypothèse nulle, la variable aléatoire $T_j$ suit une loi
        de Student à $n-p$ degrés de liberté (voir corollaire 2.2.10 du
        cours).

        On rejette $H_0$ pour un risque de première espèce $\alpha$ si
        $\left|T_j\right| > q_{1-\alpha/2}^{(\mathcal{T}_{n-p})}$. C'est
        le cas ici pour $\beta_1$ et $\beta_2$ au niveau $\alpha = 5 \%$
        : $$\begin{align*}
        \left|T_1\right| \approx 50,26 &>  q_{0,975}^{(\mathcal{T}_{1427})} \approx 1,961 \\
        \left|T_2\right| \approx 68,78 &>  q_{0,975}^{(\mathcal{T}_{1427})} \approx 1,961
        \end{align*}
        $$

    -   La quatrième colonne (colonne <tt>Pr(\>\|t\|)</tt>) contient la
        p-value pour ces mêmes tests de Student. C'est la probabilité
        pour la statistique de test sous $H_0$ de dépasser la valeur
        estimée c'est à dire explicitement :
        $$P(\mathcal{T}_{n-p}>|t|)=2\left(1-\mathcal{F}^{\mathcal{T}_{n-p}}(t)\right)$$

        De manière équivalente à la troisième colonne, ces tests de
        nullité des deux coefficients indiquent qu'ils semblent tous
        deux significativement non nuls (quand l'autre coefficient est
        fixé à la valeur estimée). En effet, les p-values sont bien
        inférieures au niveau de risque de première espèce $\alpha$
        fixé.

-   La ligne <tt>Residual standard error</tt> correspond à l'estimation
    de l'écart-type $\sigma$ des erreurs de modélisation $\varepsilon_i$
    . On lit donc $\widehat{\sigma} \approx 1,199$. Le nombre de degré
    de liberté inscrit sur la même ligne $n-p = 1~429-2 = 1~427$ car

    $$
    (n-p)\frac{\widehat{\sigma}}{\sigma} \sim \chi^2\left(n-p \right)
    $$

-   La ligne suivante donne la valeur du coefficient de détermination
    $R^2$ (<tt>Multiple R-squared</tt>). Il est définit par

    $$
    \begin{align*}
    R^2 &=\frac{SCE}{SCT}= \frac{\left\| \widehat{Y}-\overline{y}\mathbb{1}\right\|^2}{\left\| Y-\overline{y}\mathbb{1}\right\|^2} = \frac{\sum\limits_{i=1}^n \left(\widehat{y}_i-\overline{y} \right)^2}{\sum\limits_{i=1}^n \left(y_i-\overline{y} \right)^2} \\
    &= 1 - \frac{SCR}{SCT} = 1-\frac{\left\|\widehat{\varepsilon} \right\|^2}{\left\|Y - \overline{y}\mathbb{1}\right\|^2}=1-\frac{\sum\limits_{i=1}^n \widehat{\varepsilon}_i^2}{\sum\limits_{i=1}^n \left(y_i-\overline{y} \right)^2}
    \end{align*}
    $$

    On peut lire $R^2 \approx 0,77$. Cela signifie que 77 % de la
    variance des hauteurs des eucalyptus dans le jeu de données est
    expliquée par la circonférence.

    On trouve ensuite sur la même ligne le coefficient de détermination
    ajusté $R_a^2$ (<tt>Adjusted R-squared</tt>) défini par

    $$R_a^2=1-\frac{n-1}{n-p}\frac{SCR}{SCT} = 1-\frac{n-1}{n-p}\frac{\left\|\widehat{\varepsilon} \right\|^2}{\left\|Y - \overline{y}\mathbb{1}\right\|^2}=1-\frac{n-1}{n-p}\frac{\sum\limits_{i=1}^n \widehat{\varepsilon}_i^2}{\sum\limits_{i=1}^n \left(y_i-\overline{y} \right)^2}$$

    Cet ajustement permet de contrebalancer l'effet de l'ajout d'une
    variable dans la modélisation même non significative sur la valeur
    de ce coefficient. Comme $\frac{n-1}{n-p}$ est proche de 1 ici,
    $R_a^2$ et $R^2$ sont proches.

-   La dernière ligne pour la statistique de test $F$
    (<tt>F-Statistic</tt>), surtout utile en régression linéaire
    multiple, rend compte du test entre le modèle utilisé et le modèle
    ne content que la constante. Ce test permet de contrôler la
    significatiité globale du modèle. Il n'est pas très utile car il est
    équivalent au test de Student $H_0 : \beta_2 = 0$ contre
    $H_1 : \beta_2 \neq 0$.

## Question 3

Pour le modèle linéaire simple, on a :

$$
\begin{align*}
\widehat{\beta}_1 &= \overline{Y} -  \widehat{\beta}_2\overline{x} = \overline{Y} -  \frac{\textrm{Cov}(x, Y)}{\textrm{Var}(x)}\overline{x}\\
\widehat{\beta}_2 &=  \frac{\sum\limits_{i=1}^n \left(x_i-\overline{x}\right)\left(Y_i-\overline{Y}\right)}{\sum\limits_{i=1}^n \left(x_i-\overline{x}\right)^2}=\frac{\textrm{Cov}(x, Y)}{\textrm{Var}(x)} 
\end{align*}
$$


::: {.panel-tabset group="language"}

### Base R

```{webr}
#| envir: baser
#| autorun: true
y_bar <- mean(data_eucalyptus$ht)
x_bar <- mean(data_eucalyptus$circ)
cov_xy <- cov(data_eucalyptus$ht, data_eucalyptus$circ)
var_x <- var(data_eucalyptus$circ)
beta_2_hat <- cov_xy/var_x
beta_1_hat <- y_bar - beta_2_hat*x_bar
c(beta_1_hat, beta_2_hat)
```

### Tidyverse

```{webr}
#| envir: tdv
#| autorun: true
y_bar <- mean(data_eucalyptus$ht)
x_bar <- mean(data_eucalyptus$circ)
cov_xy <- cov(data_eucalyptus$ht, data_eucalyptus$circ)
var_x <- var(data_eucalyptus$circ)
beta_2_hat <- cov_xy/var_x
beta_1_hat <- y_bar - beta_2_hat*x_bar
c(beta_1_hat, beta_2_hat)
```
:::

Plus généralement pour une régression linéaire multiple, on a
matriciellement :

$$
\widehat{\beta} =  \begin{pmatrix} \widehat{\beta}_1 \\ \widehat{\beta}_2  \\ \end{pmatrix} = \left(X'X \right)^{-1}X'Y 
$$

::: {.panel-tabset group="language"}

### Base R

```{webr}
#| envir: baser
#| autorun: true
cbind(1, data_eucalyptus$circ)


```

### Tidyverse

```{webr}
#| envir: tdv
#| autorun: true
fit_lm_eucalyptus <- lm(ht ~ circ, data = data_eucalyptus) 
summary(fit_lm_eucalyptus)
```
:::