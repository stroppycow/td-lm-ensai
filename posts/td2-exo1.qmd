---
title: "TD 2 - Exercice 1"
lang: fr
author: "Théo Leroy"
date: "8 octbre 2024"
format:
  live-html:
    code-background: true
    toc: true
    page-layout: full
webr:
  render-df: gt-interactive
  resources:
    - ./../data/eucalyptus.txt
  packages:
      - dplyr
      - readr
      - ggplot2
      - cowplot
fig-align: center
editor: 
  markdown: 
    wrap: 72
---

![](/static/img/eucalyptus.jpg){fig-align="center"}

{{< include ./../_extensions/r-wasm/live/_knitr.qmd >}}

## Question 1

On charge les données du fichier `eucalyptus.txt` en R sous forme de
`data.frame`.

::: {.panel-tabset group="language"}
### Base R

```{webr}
#| envir: baser
#| autorun: true
data_eucalyptus <- read.csv(
  file = "data/eucalyptus.txt",
  header = TRUE,
  sep = " ",
  quote = '"'
)
data_eucalyptus
```

### Tidyverse

```{webr}
#| envir: tdv
#| autorun: true
data_eucalyptus <- readr::read_delim(
  file = "data/eucalyptus.txt",
  col_types = "_nnic",
  col_names = c("ht", "circ", "bloc", "clone"),
  skip = 1,
  delim = " ",
  quote = '"'
)
data_eucalyptus
```
:::

Le fichier chargé contient bien 1 429 observations avec les variables
`ht` pour la hauteur (exprimée en mètre) et `circ` pour la circonférence
des eucalyptus.

On représente ces données dans le plan.

::: {.panel-tabset group="language"}
### Base R

```{webr}
#| envir: baser
#| autorun: true
#| fig-align: center
plot(
  x = data_eucalyptus$circ,
  y = data_eucalyptus$ht,
  type ="p",
  xlab = "Circonférence",
  ylab = "Hauteur",
  pch = 3
)
grid()
```

### Tidyverse

```{webr}
#| envir: tdv
#| autorun: true
#| fig-align: center
ggplot2::ggplot(
  data = data_eucalyptus,
  mapping = aes(x = circ, y = ht)
) +
  ggplot2::labs(x = "Circonférence", y = "Hauteur") + 
  ggplot2::geom_point()
```
:::

## Question 2

On effectue la régression $y = \beta_1+\beta_2x+\varepsilon$ à l'aide de
la fonction `lm` disponible nativement dans
{{< iconify logos:r-lang >}}. On cherche ainsi une estimation de
$\beta_1$ et $\beta_2$ à l'aide de la méthode des moindres carrés
ordinaires (MCO).

::: {.panel-tabset group="language"}
### Base R

```{webr}
#| envir: baser
#| autorun: true
fit_lm_eucalyptus <- lm(ht ~ circ, data = data_eucalyptus) 
summary(fit_lm_eucalyptus)
```

### Tidyverse

```{webr}
#| envir: tdv
#| autorun: true
fit_lm_eucalyptus <- lm(ht ~ circ, data = data_eucalyptus) 
summary(fit_lm_eucalyptus)
```
:::

Commentons la sortie détaillée donnée par la fonction générique
`summary`.

-   La première partie du tableau <tt>Residuals</tt> donne quelques
    statistiques descriptives sur la distribution des résidus de la
    régression linéaire Pour rappel, le résidu $\widehat{\varepsilon}_i$
    de l'observation $i$ est la différence entre la variable à expliquer
    $Y_i$ et la valeur ajustée de $Y_i$ par le modèle c'est à dire :

    $$
    \widehat{\varepsilon}_i = Y_i- \underbrace{(\widehat{\beta}_1 + \widehat{\beta}_2X_i)}_{\widehat{Y}_i}
    $$

-   La seconde partie notée <tt>Coefficients</tt> est présentée sous
    forme de tableau. Elle nous renseigne sur l'estimation des
    coefficients et la significativité de ces derniers sous l'hypothèse
    de gaussianité des erreurs. Ce tableau comporte autant de lignes que
    de coefficient à estimer dans le modèle de régression linéaire (ce
    nombre est noté $p$ dans le cours). Ici, il y a deux lignes car deux
    coefficients interviennent dans notre modélisation, la première
    ligne correspond à l'estimation de $\beta_1$ pour l'ordonnée à
    l'origine <tt>Intercept</tt>, et la deuxième sur l'estimation de
    $\beta_2$ , l'effet de la circonférence <tt>circ</tt> sur la hauteur
    de l'arbre.

    -   La première colonne (colonne <tt>Estimate</tt>) contient les
        estimations des paramètres i.e. les estimations
        $\widehat{\beta}_1 \approx 9,03$ et
        $\widehat{\beta}_2 \approx 0,26$.

        Le coefficient signifie que lorsque

    -   La seconde colonne ( <tt>Std. Error</tt>) contient les
        écarts-types estimés des estimateurs $\widehat{\beta}_1$ et
        $\widehat{\beta}_2$ . Ils sont donnés par

        $$
        \forall\ j \in \{1, \dots, p\}, \  \widehat{\sigma}_{\widehat{\beta}_j} = \widehat{\sigma}\sqrt{(X'X)^{-1}_{j,j}} \ \textrm{avec} \ \widehat{\sigma} = \sqrt{\frac{1}{n-p}\sum\limits_{i=1}^n \widehat{\varepsilon}_i^2}
        $$

    -   Dans la troisième colonne (<tt>t. value</tt>) figure la valeur
        observée de la statistique de test de Student d'hypothèse
        $H_0 : \beta_ j = 0$ contre $H_1 : \beta_j \neq 0$.

        ::: callout-important
        Ce test est valable uniquement si sous l'hypothèse de normalité
        et indépendance des erreurs, c'est à dire si on suppose que :

        $$
        \varepsilon  = \begin{pmatrix} 
        \varepsilon_1 \\
        \vdots \\
        \varepsilon_n
        \end{pmatrix} \sim \mathcal{N}\left(0, \sigma^2 I_n \right)
        $$
        :::

        La statistique de test $T_j$ est donnée par

        $$
        T_j = \frac{{\widehat{\beta}}_j}{\widehat{\sigma}_{\widehat{\beta}_j}}
        $$

        Sous l'hypothèse nulle, la variable aléatoire $T_j$ suit une loi
        de Student à $n-p$ degrés de liberté (voir corollaire 2.2.10 du
        cours).

        On rejette $H_0$ pour un risque de première espèce $\alpha$ si
        $\left|T_j\right| > q_{1-\alpha/2}^{(\mathcal{T}_{n-p})}$. C'est
        le cas ici pour $\beta_1$ et $\beta_2$ au niveau $\alpha = 5 \%$
        : $$\begin{align*}
        \left|T_1\right| \approx 50,26 &>  q_{0,975}^{(\mathcal{T}_{1427})} \approx 1,961 \\
        \left|T_2\right| \approx 68,78 &>  q_{0,975}^{(\mathcal{T}_{1427})} \approx 1,961
        \end{align*}
        $$

    -   La quatrième colonne (colonne <tt>Pr(\>\|t\|)</tt>) contient la
        p-value pour ces mêmes tests de Student. C'est la probabilité
        pour la statistique de test sous $H_0$ de dépasser la valeur
        estimée c'est à dire explicitement :
        $$P(\mathcal{T}_{n-p}>|t|)=2\left(1-\mathcal{F}^{\mathcal{T}_{n-p}}(t)\right)$$

        De manière équivalente à la troisième colonne, ces tests de
        nullité des deux coefficients indiquent qu'ils semblent tous
        deux significativement non nuls (quand l'autre coefficient est
        fixé à la valeur estimée). En effet, les p-values sont bien
        inférieures au niveau de risque de première espèce $\alpha$
        fixé.

-   La ligne

    <td>Residual standard error</td>

    correspond à l'estimation de l'écart-type $\sigma$ des erreurs de
    modélisation $\varepsilon_i$ . On lit donc
    $\widehat{\sigma} \approx 1,199$. Le nombre de degré de liberté
    inscrit sur la même ligne $n-p = 1~429-2 = 1~427$ car

    $$
    (n-p)\frac{\widehat{\sigma}}{\sigma} \sim \chi^2\left(n-p \right)
    $$

-   La ligne suivante donne la valeur du coefficient de détermination
    $R^2$ ce coefficient est
