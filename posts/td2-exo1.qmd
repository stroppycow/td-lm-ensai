---
title: "TD 2 - Exercice 1"
lang: fr
author: "Th√©o Leroy"
date: "17 septembre 2025"
params:
  question_courante: 9
format:
  live-html:
    code-background: true
    toc: true
    page-layout: full
webr:
  render-df: gt-interactive
  resources:
    - ./../data/eucalyptus.txt
  packages:
      - dplyr
      - readr
      - ggplot2
      - cowplot
fig-align: center
filters: 
  - diagram
  - custom-callout
editor: 
  mode: source
  markdown: 
    wrap: 72
custom-callout:    
  answer:
    color: "#CCCCCC"
    icon: true
    icon-symbol: "üìù"
    appearance: "default"
    title: "Correction"
---

![](/static/img/eucalyptus.jpg){fig-align="center"}

{{< include ./../_extensions/conditionnal.qmd >}}
{{< include ./../_extensions/r-wasm/live/_knitr.qmd >}}

On veut expliquer la hauteur des eucalyptus en fonction de leur
circonf√©rence √† partir d‚Äôune r√©gression lin√©aire simple. On dispose des
mesures des hauteurs ($\texttt{ht}$) et des circonf√©rences ($\texttt{circ}$) de 1 429
eucalyptus, qui se trouvent dans le fichier `eucalyptus.txt`.

## Question 1

Extraire et repr√©senter les donn√©es dans le plan.

::: {.content-visible when-meta="is_answer_print_1"}

::: answer
On charge les donn√©es du fichier `eucalyptus.txt` en {{< iconify logos:r-lang >}} sous forme de
`data.frame`.

::: {.panel-tabset group="language"}
### Base R

```{webr}
#| envir: baser
#| autorun: true
data_eucalyptus <- read.csv(
  file = "data/eucalyptus.txt",
  header = TRUE,
  sep = " ",
  quote = '"'
)
data_eucalyptus
```

### Tidyverse

```{webr}
#| envir: tdv
#| autorun: true
data_eucalyptus <- readr::read_delim(
  file = "data/eucalyptus.txt",
  col_types = "_nnic",
  col_names = c("ht", "circ", "bloc", "clone"),
  skip = 1,
  delim = " ",
  quote = '"'
)
data_eucalyptus
```
:::

Le fichier charg√© contient bien 1¬†429 observations avec les variables
$\texttt{ht}$ pour la hauteur (exprim√©e en m√®tre) et $\texttt{circ}$ pour la circonf√©rence
des eucalyptus.

On repr√©sente ces donn√©es dans le plan.

::: {.panel-tabset group="language"}
### Base R

```{webr}
#| envir: baser
#| autorun: true
#| fig-align: center
plot(
  x = data_eucalyptus$circ,
  y = data_eucalyptus$ht,
  type ="p",
  xlab = "Circonf√©rence",
  ylab = "Hauteur",
  pch = 3
)
grid()
```

### Tidyverse

```{webr}
#| envir: tdv
#| autorun: true
#| fig-align: center
ggplot2::ggplot(
  data = data_eucalyptus,
  mapping = aes(x = circ, y = ht)
) +
  ggplot2::labs(x = "Circonf√©rence", y = "Hauteur") + 
  ggplot2::geom_point()
```
:::

Une r√©gression semble semble indiqu√©e, les points √©tant dispos√©s
grossi√®rement le long d'une droite.
:::

:::

## Question 2

Effectuer la r√©gression $y=\beta_1+\beta_2x+\epsilon$ o√π $y$ repr√©sente
la hauteur et $x$ la circonf√©rence. Commenter les r√©sultats.

::: {.content-visible when-meta="is_answer_print_2"}

::: answer
On effectue la r√©gression $y = \beta_1+\beta_2x+\varepsilon$ √† l'aide de
la fonction `lm` disponible nativement dans
{{< iconify logos:r-lang >}}. On cherche ainsi une estimation de
$\beta_1$ et $\beta_2$ √† l'aide de la m√©thode des moindres carr√©s
ordinaires (MCO).

::: {.panel-tabset group="language"}
### Base R

```{webr}
#| envir: baser
#| autorun: true
fit_lm_eucalyptus <- lm(ht ~ circ, data = data_eucalyptus) 
summary(fit_lm_eucalyptus)
```

### Tidyverse

```{webr}
#| envir: tdv
#| autorun: true
fit_lm_eucalyptus <- lm(ht ~ circ, data = data_eucalyptus) 
summary(fit_lm_eucalyptus)
```
:::

Commentons la sortie d√©taill√©e donn√©e par la fonction g√©n√©rique
`summary`.

-   La premi√®re partie du tableau <tt>Residuals</tt> donne quelques
    statistiques descriptives sur la distribution des r√©sidus de la
    r√©gression lin√©aire Pour rappel, le r√©sidu $\widehat{\varepsilon}_i$
    de l'observation $i$ est la diff√©rence entre la variable √† expliquer
    $Y_i$ et la valeur ajust√©e de $Y_i$ par le mod√®le c'est √† dire :

    $$
    \widehat{\varepsilon}_i = Y_i- \underbrace{(\widehat{\beta}_1 + \widehat{\beta}_2X_i)}_{\widehat{Y}_i}
    $$

-   La seconde partie not√©e <tt>Coefficients</tt> est pr√©sent√©e sous
    forme de tableau. Elle nous renseigne sur l'estimation des
    coefficients et la significativit√© de ces derniers sous l'hypoth√®se
    de gaussianit√© des erreurs. Ce tableau comporte autant de lignes que
    de coefficient √† estimer dans le mod√®le de r√©gression lin√©aire (ce
    nombre est not√© $p$ dans le cours). Ici, il y a deux lignes car deux
    coefficients interviennent dans notre mod√©lisation, la premi√®re
    ligne correspond √† l'estimation de $\beta_1$ pour l'ordonn√©e √†
    l'origine <tt>Intercept</tt>, et la deuxi√®me sur l'estimation de
    $\beta_2$ , l'effet de la circonf√©rence <tt>circ</tt> sur la hauteur
    de l'arbre.

    -   La premi√®re colonne (colonne <tt>Estimate</tt>) contient les
        estimations des param√®tres i.e. les estimations
        $\widehat{\beta}_1 \approx 9,03$ et
        $\widehat{\beta}_2 \approx 0,26$.

        L'estimation du coefficient estim√©t√© $\widehat{\beta}_2$
        interpr√®te de la mani√®re suivante : "Lorsque la circonf√©rence
        d'un arbre augmente d'un centim√®tre sa hauteur gagne 26
        centim√®tres."

    -   La seconde colonne ( <tt>Std. Error</tt>) contient les
        √©carts-types estim√©s des estimateurs $\widehat{\beta}_1$ et
        $\widehat{\beta}_2$ . Ils sont donn√©s par

        $$
        \forall\ j \in \{1, \dots, p\}, \  \widehat{\sigma}_{\widehat{\beta}_j} = \widehat{\sigma}\sqrt{(X'X)^{-1}_{j,j}} \ \textrm{avec} \ \widehat{\sigma} = \sqrt{\frac{1}{n-p}\sum\limits_{i=1}^n \widehat{\varepsilon}_i^2}
        $$

    -   Dans la troisi√®me colonne (<tt>t. value</tt>) figure la valeur
        observ√©e de la statistique de test de Student d'hypoth√®se
        $H_0 : \beta_ j = 0$ contre $H_1 : \beta_j \neq 0$.

        ::: callout-important
        Ce test exact est valable uniquement sous l'hypoth√®se de
        normalit√© et ind√©pendance des erreurs, c'est √† dire si on
        suppose que :

        $$
        \varepsilon  = \begin{pmatrix} 
        \varepsilon_1 \\
        \vdots \\
        \varepsilon_n
        \end{pmatrix} \sim \mathcal{N}\left(0, \sigma^2 I_n \right)
        $$
        :::

        La statistique de test $T_j$ est donn√©e par

        $$
        T_j = \frac{{\widehat{\beta}}_j}{\widehat{\sigma}_{\widehat{\beta}_j}}
        $$

        Sous l'hypoth√®se nulle, la variable al√©atoire $T_j$ suit une loi
        de Student √† $n-p$ degr√©s de libert√© (voir corollaire 2.2.10 du
        cours).

        On rejette $H_0$ pour un risque de premi√®re esp√®ce $\alpha$ si
        $\left|T_j\right| > q_{1-\alpha/2}^{(\mathcal{T}_{n-p})}$. C'est
        le cas ici pour $\beta_1$ et $\beta_2$ au niveau $\alpha = 5 \%$
        : $$\begin{align*}
        \left|T_1\right| \approx 50,26 &>  q_{0,975}^{(\mathcal{T}_{1427})} \approx 1,961 \\
        \left|T_2\right| \approx 68,78 &>  q_{0,975}^{(\mathcal{T}_{1427})} \approx 1,961
        \end{align*}
        $$

    -   La quatri√®me colonne (colonne <tt>Pr(\>\|t\|)</tt>) contient la
        p-value pour ces m√™mes tests de Student. C'est la probabilit√©
        pour la statistique de test sous $H_0$ de d√©passer la valeur
        estim√©e c'est √† dire explicitement :
        $$P(\mathcal{T}_{n-p}>|t|)=2\left(1-\mathcal{F}^{\mathcal{T}_{n-p}}(t)\right)$$

        De mani√®re √©quivalente √† la troisi√®me colonne, ces tests de
        nullit√© des deux coefficients indiquent qu'ils semblent tous
        deux significativement non nuls (quand l'autre coefficient est
        fix√© √† la valeur estim√©e). En effet, les p-values sont bien
        inf√©rieures au niveau de risque de premi√®re esp√®ce $\alpha$
        fix√©.

-   La ligne <tt>Residual standard error</tt> correspond √† l'estimation
    de l'√©cart-type $\sigma$ des erreurs de mod√©lisation $\varepsilon_i$
    . On lit donc $\widehat{\sigma} \approx 1,199$. Le nombre de degr√©
    de libert√© inscrit sur la m√™me ligne $n-p = 1~429-2 = 1~427$ car

    $$
    (n-p)\frac{\widehat{\sigma}}{\sigma} \sim \chi^2\left(n-p \right)
    $$

-   La ligne suivante donne la valeur du coefficient de d√©termination
    $R^2$ (<tt>Multiple R-squared</tt>). Il est d√©finit par

    $$
    \begin{align*}
    R^2 &=\frac{SCE}{SCT}= \frac{\left\| \widehat{Y}-\overline{y}\mathbb{1}\right\|^2}{\left\| Y-\overline{y}\mathbb{1}\right\|^2} = \frac{\sum\limits_{i=1}^n \left(\widehat{y}_i-\overline{y_n} \right)^2}{\sum\limits_{i=1}^n \left(y_i-\overline{y_n} \right)^2} \\
    &= 1 - \frac{SCR}{SCT} = 1-\frac{\left\|\widehat{\varepsilon} \right\|^2}{\left\|Y - \overline{y_n}\mathbb{1}\right\|^2}=1-\frac{\sum\limits_{i=1}^n \widehat{\varepsilon}_i^2}{\sum\limits_{i=1}^n \left(y_i-\overline{y_n} \right)^2}
    \end{align*}
    $$

    On peut lire $R^2 \approx 0,77$. Cela signifie que 77 % de la
    variance des hauteurs des eucalyptus dans le jeu de donn√©es est
    expliqu√©e par la circonf√©rence.

    On trouve ensuite sur la m√™me ligne le coefficient de d√©termination
    ajust√© $R_a^2$ (<tt>Adjusted R-squared</tt>) d√©fini par

    $$R_a^2=1-\frac{n-1}{n-p}\frac{SCR}{SCT} = 1-\frac{n-1}{n-p}\frac{\left\|\widehat{\varepsilon} \right\|^2}{\left\|Y - \overline{y_n}\mathbb{1}\right\|^2}=1-\frac{n-1}{n-p}\frac{\sum\limits_{i=1}^n \widehat{\varepsilon}_i^2}{\sum\limits_{i=1}^n \left(y_i-\overline{y_n} \right)^2}$$

    Cet ajustement permet de contrebalancer l'effet de l'ajout d'une
    variable dans la mod√©lisation m√™me non significative sur la valeur
    de ce coefficient. Comme $\frac{n-1}{n-p}$ est proche de 1 ici,
    $R_a^2$ et $R^2$ sont proches.

-   La derni√®re ligne pour la statistique de test $F$
    (<tt>F-Statistic</tt>), surtout utile en r√©gression lin√©aire
    multiple, rend compte du test entre le mod√®le complet et le mod√®le
    ne contenant que la constante. Ce test permet de contr√¥ler la
    significativit√© globale du mod√®le. Il n'est pas tr√®s utile ici car
    il est √©quivalent au test de Student $H_0 : \beta_2 = 0$ contre
    $H_1 : \beta_2 \neq 0$.
:::

:::

## Question 3

Quelle est la formule th√©orique de $\hat{\beta}_1$ et $\hat{\beta}_2$ ?
Retrouver les estimations fournies par {{< iconify logos:r-lang >}} en l‚Äôutilisant.

::: {.content-visible when-meta="is_answer_print_3"}

::: answer
Pour le mod√®le lin√©aire simple, on a :

$$
\begin{align*}
\widehat{\beta}_1 &= \overline{Y_n} -  \widehat{\beta}_2\overline{x_n} = \overline{Y}_n -  \frac{\textrm{Cov}(x, Y)}{\textrm{Var}(x)}\overline{x_n}\\
\widehat{\beta}_2 &=  \frac{\sum\limits_{i=1}^n \left(x_i-\overline{x_n}\right)\left(Y_i-\overline{Y_n}\right)}{\sum\limits_{i=1}^n \left(x_i-\overline{x_n}\right)^2}=\frac{\textrm{Cov}(x, Y)}{\textrm{Var}(x)} 
\end{align*}
$$

::: {.panel-tabset group="language"}
### Base R

```{webr}
#| envir: baser
#| autorun: true
y_bar <- mean(data_eucalyptus$ht)
x_bar <- mean(data_eucalyptus$circ)
cov_xy <- cov(data_eucalyptus$ht, data_eucalyptus$circ)
var_x <- var(data_eucalyptus$circ)
beta_2_hat <- cov_xy/var_x
beta_1_hat <- y_bar - beta_2_hat*x_bar
matrix(data = c(beta_1_hat, beta_2_hat), ncol = 1)
```

### Tidyverse

```{webr}
#| envir: tdv
#| autorun: true
y_bar <- mean(data_eucalyptus$ht)
x_bar <- mean(data_eucalyptus$circ)
cov_xy <- cov(data_eucalyptus$ht, data_eucalyptus$circ)
var_x <- var(data_eucalyptus$circ)
beta_2_hat <- cov_xy/var_x
beta_1_hat <- y_bar - beta_2_hat*x_bar
matrix(data = c(beta_1_hat, beta_2_hat), ncol = 1)
```
:::

Les r√©sultats sont coh√©rents avec la sortie {{< iconify logos:r-lang >}}
de la fonction `lm`.

On pouvait raisonner aussi matriciellement . Plus g√©n√©ralement pour une
r√©gression lin√©aire multiple, on a :

$$
\widehat{\beta} =  \begin{pmatrix} \widehat{\beta}_1 \\ \widehat{\beta}_2  \\ \end{pmatrix} = \left(X'X \right)^{-1}X'Y 
$$

::: {.panel-tabset group="language"}
### Base R

```{webr}
#| envir: baser
#| autorun: true
X <- cbind(1, data_eucalyptus$circ)
Y <- matrix(data = data_eucalyptus$ht, ncol=1)
solve(t(X)%*%X)%*%t(X)%*%Y


```

### Tidyverse

```{webr}
#| envir: tdv
#| autorun: true
X <- cbind(1, data_eucalyptus$circ)
Y <- matrix(data = data_eucalyptus$ht, ncol=1)
solve(t(X)%*%X)%*%t(X)%*%Y
```
:::

On retrouve les m√™mes valeurs.
:::

:::

## Question 4

Calculer un intervalle de confiance √† 95 % pour $\beta_1$ et $\beta_2$,
en supposant la normalit√© des donn√©es.

::: {.content-visible when-meta="is_answer_print_4"}

::: answer
La formule de l'intervalle de confiance pour le coefficient $\beta_j$
sous l'hypoth√®se d'erreurs gaussiennes, centr√©es et homosc√©dastiques est
:
$$
I C_{1-\alpha}\left(\beta_j\right)=\left[\hat{\beta}_j \pm q_{1-\alpha/2}^{\mathcal{T}_{n-p}} \hat{\sigma}_{\hat{\beta}_j}\right]
$$

o√π $\hat{\sigma}_{\hat{\beta}_j}=\hat{\sigma} \sqrt{\left(X^{\prime} X\right)_{j j}^{-1}}$
est l'estimateur de l'√©cart-type de $\hat{\beta}_j$.

En reportant les r√©sultats du mod√®le de r√©gression, on obtient pour
$\alpha=0,05\ \ %$

::: {.panel-tabset group="language"}
### Base R

```{webr}
#| envir: baser
#| autorun: true
beta_1_hat <- 9.037476
beta_2_hat <- 0.257138
sigma_beta_1_hat <- 0.179802
sigma_beta_2_hat <- 0.003738
qt975 <- qt(p = 0.975, df = nrow(data_eucalyptus)-2L)
matrix(
  data = c(
    beta_1_hat - qt975*sigma_beta_1_hat,
    beta_1_hat + qt975*sigma_beta_1_hat,
    beta_2_hat - qt975*sigma_beta_2_hat,
    beta_2_hat + qt975*sigma_beta_2_hat
  ),
  nrow = 2L,
  ncol = 2L,
  byrow = TRUE,
  dimnames = list(
    c("beta_1", "beta_2"),
    c("Borne inf√©rieure de l'IC", "Borne sup√©rieure de l'IC")
  )
)
```

### Tidyverse

```{webr}
#| envir: tdv
#| autorun: true
beta_1_hat <- 9.037476
beta_2_hat <- 0.257138
sigma_beta_1_hat <- 0.179802
sigma_beta_2_hat <- 0.003738
qt975 <- qt(p = 0.975, df = nrow(data_eucalyptus)-2L)
matrix(
  data = c(
    beta_1_hat - qt975*sigma_beta_1_hat,
    beta_1_hat + qt975*sigma_beta_1_hat,
    beta_2_hat - qt975*sigma_beta_2_hat,
    beta_2_hat + qt975*sigma_beta_2_hat
  ),
  nrow = 2L,
  ncol = 2L,
  byrow = TRUE,
  dimnames = list(
    c("beta_1", "beta_2"),
    c("Borne inf√©rieure de l'IC", "Borne sup√©rieure de l'IC")
  )
)
```
:::

On peut aussi appliquer cette formule en r√©cup√©rant les coefficients
estim√©s automatiquement √† partir de l'objet {{< iconify logos:r-lang >}}
de l'estimation du mod√®le en sortie de la fonction `lm`.

::: {.panel-tabset group="language"}
### Base R

```{webr}
#| envir: baser
#| autorun: true
beta_1_hat <- fit_lm_eucalyptus$coefficients[1]
beta_2_hat <- fit_lm_eucalyptus$coefficients[2]
sigma_beta_1_hat <- sqrt(vcov(fit_lm_eucalyptus)[1, 1])
sigma_beta_2_hat <- sqrt(vcov(fit_lm_eucalyptus)[2, 2])
qt975 <- qt(p = 0.975, df = nrow(data_eucalyptus)-2L)
matrix(
  data = c(
    beta_1_hat - qt975*sigma_beta_1_hat,
    beta_1_hat + qt975*sigma_beta_1_hat,
    beta_2_hat - qt975*sigma_beta_2_hat,
    beta_2_hat + qt975*sigma_beta_2_hat
  ),
  nrow = 2L,
  ncol = 2L,
  byrow = TRUE,
  dimnames = list(
    c("beta_1", "beta_2"),
    c("Borne inf√©rieure de l'IC", "Borne sup√©rieure de l'IC")
  )
)
```

### Tidyverse

```{webr}
#| envir: tdv
#| autorun: true
beta_1_hat <- fit_lm_eucalyptus$coefficients[1]
beta_2_hat <- fit_lm_eucalyptus$coefficients[2]
sigma_beta_1_hat <- sqrt(vcov(fit_lm_eucalyptus)[1, 1])
sigma_beta_2_hat <- sqrt(vcov(fit_lm_eucalyptus)[2, 2])
qt975 <- qt(p = 0.975, df = nrow(data_eucalyptus)-2L)
matrix(
  data = c(
    beta_1_hat - qt975*sigma_beta_1_hat,
    beta_1_hat + qt975*sigma_beta_1_hat,
    beta_2_hat - qt975*sigma_beta_2_hat,
    beta_2_hat + qt975*sigma_beta_2_hat
  ),
  nrow = 2L,
  ncol = 2L,
  byrow = TRUE,
  dimnames = list(
    c("beta_1", "beta_2"),
    c("Borne inf√©rieure de l'IC", "Borne sup√©rieure de l'IC")
  )
)
```
:::

On peut aussi utiliser directement la fonction `confint`.

::: {.panel-tabset group="language"}
### Base R

```{webr}
#| envir: baser
#| autorun: true
confint(fit_lm_eucalyptus)
```

### Tidyverse

```{webr}
#| envir: tdv
#| autorun: true
confint(fit_lm_eucalyptus)
```
:::

Quelle que soit la m√©thode, on trouve :

$$
\begin{align*}
I C_{95\ \%}\left(\beta_1\right)&=\left[8,685\ ;\ 9,390\right] \\
I C_{95\ \%}\left(\beta_2\right)&=\left[0,250\ ;\ 0,264\right]
\end{align*}
$$
:::

:::

## Question 5

Si le bruit $\epsilon$ ne suit pas une loi normale, les intervalles de
confiance pr√©c√©dents restent-ils valables ?

::: {.content-visible when-meta="is_answer_print_5"}

::: answer
Pour que ces intervalles de confiances soient exacts, l'erreur doit √™tre
gausienne. Cependant, ils le deviennent asymptotiquement sous certaines
conditions assez faible de r√©gularit√© gr√¢ce au th√©or√®me central limite
(cf. remarque slide 70 du cours). Comme, ici $n=1\ 429 \gg 30$ , on peut
se fier √† ces intervalles de confiance m√™me si la normalit√© des erreurs
n'est pas v√©rifi√©e.
:::

:::

## Question 6

Tracer l‚Äôestimateur de la droite de r√©gression et un intervalle de
confiance √† 95 % de celle-ci. Que d√©duisez-vous de la qualit√© de
l‚Äôestimation ?

::: {.content-visible when-meta="is_answer_print_6"}

::: answer
::: {.panel-tabset group="language"}
### Base R

```{webr}
#| envir: baser
#| autorun: true
#| fig-align: center
plot(
  x = data_eucalyptus$circ,
  y = data_eucalyptus$ht,
  type = "p",
  xlab = "Circonf√©rence",
  ylab = "Hauteur",
  pch = 3,
  col = "grey60"
)
abline(fit_lm_eucalyptus, col = "red")
newdata <- data.frame(circ = seq(from = 25, to = 70, by = 0.1))
confint_val <- predict(
  object = fit_lm_eucalyptus,
  newdata = newdata,
  interval = "confidence"
)

lines(newdata$circ, confint_val[, "lwr"], col = "red", lty = 2)
lines(newdata$circ, confint_val[, "upr"], col = "red", lty = 2)
grid()

```

### Tidyverse

```{webr}
#| envir: tdv
#| autorun: true
#| fig-align: center
newdata <- data.frame(circ = seq(from = 25, to = 80, by = 0.1))
confint_val <- predict(
  object = fit_lm_eucalyptus,
  newdata = newdata,
  interval = "confidence"
) |>
  tibble::as_tibble() |>
  cbind(newdata)

ggplot2::ggplot() +
  ggplot2::labs(x = "Circonf√©rence", y = "Hauteur") + 
  ggplot2::geom_point(
    data = data_eucalyptus,
    mapping = aes(x = circ, y = ht),
    col = "grey70"
  ) + 
 ggplot2::geom_ribbon(
   data = confint_val,
   mapping = aes(ymin = lwr, ymax = upr, x = circ),
   fill = "red",
   alpha = 0.5
 ) +
 ggplot2::geom_abline(
    intercept = fit_lm_eucalyptus$coefficients[1],
    slope = fit_lm_eucalyptus$coefficients[2],
    col = "red"
  ) + 
 ggplot2::theme_light()
```
:::

On constate une faible incertitude sur la droite estim√©e.
:::

:::

## Question 7

On veut √† pr√©sent pr√©dire la hauteur d‚Äôune nouvelle s√©rie d‚Äôeucalyptus
de circonf√©rences 50, 100, 150 et 200. Donner les estimateurs de la
taille de chacun d‚Äôentre eux et les intervalles de pr√©vision √† 95 %
associ√©s, en supposant la normalit√© des donn√©es.

::: {.content-visible when-meta="is_answer_print_7"}

::: answer
La pr√©vision $\widehat{y}_{n+1}^p$ pour une circonf√©rence $x_{n+1}$ est
donn√©e par la formule
$$\widehat{y}_{n+1}^p = \widehat{\beta}_1+\widehat{\beta}_2x_{n+1}$$

Sous l'hypoth√®se de gaussianit√© des erreurs, un IC de $y_{n+1}$ est
donn√© par

$$
I C_{1-\alpha}\left(y_{n+1}\right)=\left[\widehat{y}_{n+1}^p \pm q_{1-\alpha/2}^{\mathcal{T}_{n-p}} \hat{\sigma}\sqrt{1+\frac{1}{n}+\frac{\left(x_{n+1}-\overline{x_n}\right)^2}{\sum\limits_{i=1}^n \left(x_i - \overline{x_n} \right)^2}}\right]
$$

::: {.panel-tabset group="language"}
### Base R

```{webr}
#| envir: baser
#| autorun: true
#| fig-align: center
n <- nrow(data_eucalyptus)
p <- 2L
beta_1_hat <- fit_lm_eucalyptus$coefficients[1] 
beta_2_hat <- fit_lm_eucalyptus$coefficients[2]
x_bar <- mean(data_eucalyptus$circ)
var_x <- var(data_eucalyptus$circ)
q975 <- qt(p = 0.975, df = n-p)
sigma_hat <- sigma(fit_lm_eucalyptus)

newdata <- data.frame(circ = c(50, 100, 150, 200))
newdata$hauteurs_predites <- beta_1_hat + newdata$circ*beta_2_hat
newdata$pm <- q975*sigma_hat*sqrt(1+1/n+(newdata$circ-x_bar)**2/(n*var_x))
newdata$borne_inf_pred <- newdata$hauteurs_predites - newdata$pm
newdata$borne_sup_pred <- newdata$hauteurs_predites + newdata$pm
newdata$pm <- NULL
newdata
```

### Tidyverse

```{webr}
#| envir: tdv
#| autorun: true
#| fig-align: center
n <- nrow(data_eucalyptus)
p <- 2L
beta_1_hat <- fit_lm_eucalyptus$coefficients[1] 
beta_2_hat <- fit_lm_eucalyptus$coefficients[2]
x_bar <- mean(data_eucalyptus$circ)
var_x <- var(data_eucalyptus$circ)
q975 <- qt(p = 0.975, df = n-p)
sigma_hat <- sigma(fit_lm_eucalyptus)

newdata <- tibble::tibble(circ = c(50, 100, 150, 200)) |>
  dplyr::mutate(
    hauteurs_predites = beta_1_hat + circ*beta_2_hat,
    pm = q975*sigma_hat*sqrt(1+1/n+(circ-x_bar)**2/(n*var_x)),
    borne_inf_pred = hauteurs_predites - pm,
    borne_sup_pred = hauteurs_predites + pm 
) |>
  dplyr::select(-pm)

newdata
```
:::

On peut obtenir ces r√©sultats plus simplement avec la fonction `predict`
avec l'option `interval="prediction"` qui permet de calculer les
intervalles de pr√©vision autour des pr√©visions.

::: {.panel-tabset group="language"}
### Base R

```{webr}
#| envir: baser
#| autorun: true
#| fig-align: center

newdata <- data.frame(circ = c(50, 100, 150, 200))
predict(
  object = fit_lm_eucalyptus,
  newdata = newdata,
  interval = "prediction"
) |> as.data.frame()
```

### Tidyverse

```{webr}
#| envir: tdv
#| autorun: true
#| fig-align: center
newdata <- tibble::tibble(circ = c(50, 100, 150, 200))
predict(
  object = fit_lm_eucalyptus,
  newdata = newdata,
  interval = "prediction"
) |> tibble::as_tibble()
```
:::

On remarque que les intervalles de pr√©vision sont plus √©lev√©s pour les
valeurs de circonf√©rences loin de la moyenne des donn√©es. Pour rappel
l‚Äôerreur de pr√©vision cumule deux erreurs :

-   l‚Äôerreur due √† l‚Äôestimation de la droite (celle-ci est moins pr√©cise
    loin de la moyenne des donn√©es) ;
-   l‚Äôerreur due √† $\varepsilon_{n+1}$: elle est stable quelle que soit
    la valeur de $\texttt{circ}$ et incompressible car
    $\mathrm{Var}(\varepsilon_{n+1})=\sigma^2$.
:::

:::

## Question 8

Ajouter √† la repr√©sentation graphique de la question 6 les intervalles
de pr√©vision (associ√©s aux m√™mes valeurs de la circonf√©rence).

::: {.content-visible when-meta="is_answer_print_8"}

::: answer
::: {.panel-tabset group="language"}
### Base R

```{webr}
#| envir: baser
#| autorun: true
#| fig-align: center
plot(
  x = data_eucalyptus$circ,
  y = data_eucalyptus$ht,
  type = "p",
  xlab = "Circonf√©rence",
  ylab = "Hauteur",
  pch = 3,
  col = "grey60"
)
abline(fit_lm_eucalyptus, col = "red")
newdata <- data.frame(circ = seq(from = 25, to = 70, by = 0.1))
confint_val <- predict(
  object = fit_lm_eucalyptus,
  newdata = newdata,
  interval = "confidence"
)

lines(newdata$circ, confint_val[, "lwr"], col = "red", lty = 2)
lines(newdata$circ, confint_val[, "upr"], col = "red", lty = 2)

predint_val <- predict(
  object = fit_lm_eucalyptus,
  newdata = newdata,
  interval = "prediction"
)

lines(newdata$circ, predint_val[, "lwr"], col = "blue", lty = 2)
lines(newdata$circ, predint_val[, "upr"], col = "blue", lty = 2)
grid()

```

### Tidyverse

```{webr}
#| envir: tdv
#| autorun: true
#| fig-align: center
newdata <- data.frame(circ = seq(from = 25, to = 80, by = 0.1))
confint_val <- predict(
  object = fit_lm_eucalyptus,
  newdata = newdata,
  interval = "confidence"
) |>
  tibble::as_tibble() |>
  cbind(newdata)
predint_val <- predict(
  object = fit_lm_eucalyptus,
  newdata = newdata,
  interval = "prediction"
) |>
  tibble::as_tibble() |>
  cbind(newdata)


ggplot2::ggplot() +
  ggplot2::labs(x = "Circonf√©rence", y = "Hauteur") + 
  ggplot2::geom_point(
    data = data_eucalyptus,
    mapping = aes(x = circ, y = ht),
    col = "grey70"
  ) + 
 ggplot2::geom_ribbon(
   data = predint_val,
   mapping = aes(ymin = lwr, ymax = upr, x = circ),
   fill = "blue",
   alpha = 0.5
 ) +
 ggplot2::geom_ribbon(
   data = confint_val,
   mapping = aes(ymin = lwr, ymax = upr, x = circ),
   fill = "red",
   alpha = 0.5
 ) +
 ggplot2::geom_abline(
    intercept = fit_lm_eucalyptus$coefficients[1],
    slope = fit_lm_eucalyptus$coefficients[2],
    col = "red"
  ) + 
 ggplot2::theme_light()
```
:::

:::

:::

## Question 9

Si le bruit $\epsilon$ ne suit pas une loi normale, les intervalles de
pr√©vision pr√©c√©dents restent-ils valables ?

::: {.content-visible when-meta="is_answer_print_9"}

::: answer
Non, les intervalles de confiance pour les pr√©visions ne sont plus du
tout valable si l'hypoth√®se Gaussienne n'est plus v√©rifi√©e. Comme nous
l'avons mentionn√© √† la question 6, l‚Äôerreur de pr√©vision cumule deux
erreurs :

-   l‚Äôerreur due √† l‚Äôestimation de la droite i.e. l'erreur li√©e √†
    $\widehat{\beta}$. Cet estimateur suit asymptotiquement une loi
    gaussienne m√™me si les erreurs ne sont pas gaussienne.
-   l‚Äôerreur due √† $\varepsilon_{n+1}$ : cette erreur suit sa propre loi
    et il n'y a pas de convergence lorsque $n$ augmente.

√Ä cause de cette deuxi√®me erreur, $Y_{n+1}-\widehat{Y}_{n+1}$ ne suit
pas une loi Gaussienne et donc les intervalles de confiance de la
pr√©diction ne sont plus valables.
:::

:::
