---
title: "TD 1 - Exercice 4"
lang: fr
author: "Théo Leroy"
date: "3 septembre 2025"
format:
  live-html:
    code-background: true
    toc: true
    page-layout: full
webr:
  render-df: gt-interactive
fig-align: center
filters: 
  - diagram
  - custom-callout
editor: 
  markdown: 
    wrap: 72
diagram:
  engine:
    tikz:
      execpath: lualatex
      header-includes:
        - '\usepackage{tkz-tab}'
custom-callout:    
  answer:
    color: "#CCCCCC"
    appearance: "minimal"
    title: ""
---

{{< include ./../_extensions/r-wasm/live/_knitr.qmd >}}

On considère le modèle de régression linéaire simple où l’on observe $n$
réalisations $(x_i, y_i)_{i \in [\![1,n]\!]}$; liées par la relation
$y_i = \beta_0 + \beta_1x_i + \epsilon_i$; $i = 1, \ \dots, \ n$.

## Question 1

Quelle sont les hypothèses standards sur les erreurs de modélisation
$\epsilon_i$ ?

::: answer
On suppose que les variables $\epsilon_i$ sont :

-   centrées i.e.
    $\forall\ i \in [\![1,n]\!], \ \mathbb{E}(\epsilon_i)=0$,
-   homoscédastiques i.e.
    $\exists\  \sigma^2 \in \mathbb{R}, \ \forall\ i \in [\![1,n]\!], \ \mathbb{V}(\epsilon_i)=\sigma^2$,
-   et non-corrélées deux à deux i.e
    $\forall\ (i,j) \in [\![1,n]\!]^2, i \neq j, \ \mathbb{C}\mathbb{o}\mathbb{v}(\epsilon_i,\epsilon_j)=0$.
:::

## Question 2

Sous quelle hypothèse le modèle est-il identifiable, au sens où
$\beta_0$ et $\beta_1$ sont définis de manière unique ?

::: answer
Il faut que les variables explicatives ne soient pas linéairement liées.
En effet, si $\textrm{rang}(X) < p$ (i.e. il existe des colonnes
linéairement dépendantes) : plusieurs vecteurs $\beta$ différents
produisent le même ajustement $X\beta$. Dans ce cas, le modèle n’est pas
identifiable.

Pour le modèle de régression linéaire simple $\textrm{rang}(X)=2$
signifie le vecteur du régresseur $(x_1, \dots, x_n)$ n'est pas
proportionnel au vecteur constant $(1, \dots, 1)$ : c'est à dire que les
$x_i$ ne prennent pas tous la même valeur.
:::

## Question 3

Sous quelle hypothèse l’estimateur par MCO de $\beta_0$ et $\beta_1$
existe-t-il ?

:::: answer
C'est la même hypothèse que pour la question précédente, elle garantie
l'inversibilité de $X'X$.

::: {.callout-note collapse="true"}
### Proposition : Inversibilité de $X'X$

Soit $X \in \mathbb{R}^{n \times p}$. Alors $X'X$ est inversible si et
seulement si $X$ est de rang plein, c'est-à-dire
$\operatorname{rang}(X) = p$.

**Preuve**

[Sens direct ($\Rightarrow$)]{.underline}

Supposons que $X' X$ soit inversible.\
Si $\operatorname{rang}(X) < p$, alors il existe
$v \in \mathbb{R}^p \setminus \{0\}$ tel que $Xv = 0$.\
On aurait alors : $$
X^\top X v = X^\top (Xv) = X^\top 0 = 0,
$$ ce qui montre que $v$ est un vecteur non nul du noyau de $X'X$.\
C'est absurde car cela contredit l’inversibilité de $X'X$. Donc
nécessairement $\operatorname{rang}(X) = p$.

[Sens réciproque ($\Leftarrow$)]{.underline}

Supposons maintenant que $\operatorname{rang}(X) = p$, i.e. les colonnes
de $X$ sont linéairement indépendantes.\
Prenons $v \in \mathbb{R}^p$ tel que $X^\top X v = 0$.\
Alors : $$
0 = v' (X' X v) = (Xv)'(Xv) = \|Xv\|_2^2
$$ Ainsi, $Xv = 0$. Or, comme $X$ est de rang plein, son noyau est
réduit à $\{0\}$, donc $v=0$.\
Cela montre que le noyau de $X' X$ est trivial, donc $X' X$ est
inversible.
:::
::::

## Question 4

Les variables $y_i$ ont-elles même espérance ?

::: answer
Calculons l'espérance de la variable aléatoire $y_i$. on suppose les
$x_i$ déterministes pour simplifier la notation sinon il faut raisonner
avec des espérances conditonnelles et supposer que les variables
$\epsilon_i$ et $x_i$ sont indépendantes quel que soit
$i \in [\![1,n]\!]$.

Par linéarité de l'espérance, on a :
$$\mathbb{E}(y_i)=\mathbb{E}(\beta_0+\beta_1x_i+\epsilon_i)=\mathbb{E}(\beta_0)+\mathbb{E}(\beta_1x_i)+\mathbb{E}(\epsilon_i)$$
Or

-   $\mathbb{E}(\beta_0)=\beta_0$ et $\mathbb{E}(\beta_1x_i)=\beta_1x_i$
    car $\beta_0$, $\beta_1$ et $x_i$ sont déterministes ;
-   $\mathbb{E}(\epsilon_i)=0$ par hypothèse du modèle.

Donc $$\mathbb{E}(y_i)=\beta_0+\beta_1x_i$$

Les variables $y_i$ n'ont pas la même espérance sauf si $\beta_1=0$.
:::

## Question 5

La droite de régression estimée à partir des observations
$(x_i; y_i)_{i \in [\![1,n]\!]}$ passe-t-elle toujours par le point
$(\overline{x}; \overline{y})$ ?

::: answer
Oui (*cf question 5 de l'exercice 3*)
:::

## Question 6

Les estimateurs par MCO des coefficients $\hat{\beta}_0$
et$\hat{\beta}_1$ sont-ils indépendants ?

::: answer
Montrons que les estimateurs par MCO des coefficients $\hat{\beta}_0$
et$\hat{\beta}_1$ ne sont pas indépendants. Étudions la matrice de
variance-covariance de $\hat{\beta}$ :

$$\mathbb{V}(\hat{\beta})=\sigma^2(X'X)^{-1}=\frac{\sigma^2}{n\textrm{var}(x)}\begin{pmatrix}
\overline{x^2} & -\overline{x} \\
-\overline{x} & 1
\end{pmatrix}$$

Cette matrice n'est pas nécessairement diagonale (il faut que
$\overline{x}=0$) et même dans ce cas cela ne garantit pas
l’indépendance sans hypothèse gaussienne.
:::

## Question 7

Est-il possible de trouver des estimateurs des coefficients de
régression de plus faible variance que celle des estimateurs par MCO ?

::: answer
Oui, en particulier si on accepte du biais, on peut trouver des
estimateurs de variance plus faible.

On peut considérer $\hat{\beta}= \frac{1}{2}\hat{\beta}_{\textrm{MCO}}$
par exemple.
:::
