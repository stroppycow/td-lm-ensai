---
title: "TD 1 - Exercice 4"
lang: fr
author: "Th√©o Leroy"
date: "3 septembre 2025"
params:
  question_courante: 7
format:
  live-html:
    code-background: true
    toc: true
    page-layout: full
webr:
  render-df: gt-interactive
fig-align: center
filters: 
  - custom-callout
editor: 
  markdown: 
    wrap: 72
  mode: source
custom-callout:    
  answer:
    color: "#CCCCCC"
    icon: true
    icon-symbol: "üìù"
    appearance: "default"
    title: "Correction"
---

{{< include ./../_extensions/conditionnal.qmd >}}
{{< include ./../_extensions/r-wasm/live/_knitr.qmd >}}

On consid√®re le mod√®le de r√©gression lin√©aire simple o√π l‚Äôon observe $n$
r√©alisations $(x_i, y_i)_{i \in [\![1,n]\!]}$; li√©es par la relation
$$
y_i = \beta_0 + \beta_1x_i + \epsilon_i \ ; \quad i = 1, \ \dots, \ n \textrm{.}
$$

## Question 1

Quelle sont les hypoth√®ses standards sur les erreurs de mod√©lisation
$\epsilon_i$ ?

::: {.content-visible when-meta="is_answer_print_1"}

::: answer
On suppose que les variables $\epsilon_i$ sont :

-   centr√©es i.e.
    $\forall\ i \in [\![1,n]\!], \ \mathbb{E}(\epsilon_i)=0$,
-   homosc√©dastiques i.e.
    $\exists\  \sigma^2 \in \mathbb{R}, \ \forall\ i \in [\![1,n]\!], \ \mathbb{V}(\epsilon_i)=\sigma^2$,
-   et non-corr√©l√©es deux √† deux i.e
    $\forall\ (i,j) \in [\![1,n]\!]^2, i \neq j, \ \mathbb{C}\mathbb{o}\mathbb{v}(\epsilon_i,\epsilon_j)=0$.
:::

:::

## Question 2

Sous quelle hypoth√®se le mod√®le est-il identifiable, au sens o√π
$\beta_0$ et $\beta_1$ sont d√©finis de mani√®re unique ?

::: {.content-visible when-meta="is_answer_print_2"}

::: answer
Il faut que les variables explicatives ne soient pas lin√©airement li√©es.
En effet, si $\textrm{rang}(X) < p$ (i.e. il existe des colonnes
lin√©airement d√©pendantes) : plusieurs vecteurs $\beta$ diff√©rents
produisent le m√™me ajustement $X\beta$. Dans ce cas, le mod√®le n‚Äôest pas
identifiable.

Pour le mod√®le de r√©gression lin√©aire simple $\textrm{rang}(X)=2$
signifie le vecteur du r√©gresseur $(x_1, \dots, x_n)$ n'est pas
proportionnel au vecteur constant $(1, \dots, 1)$ : c'est √† dire que les
$x_i$ ne prennent pas tous la m√™me valeur.
:::

:::

## Question 3

Sous quelle hypoth√®se l‚Äôestimateur par MCO de $\beta_0$ et $\beta_1$
existe-t-il ?

::: {.content-visible when-meta="is_answer_print_3"}

::: answer
C'est la m√™me hypoth√®se que pour la question pr√©c√©dente, elle garantie
l'inversibilit√© de $X'X$.

::: {.callout-note collapse="true"}
### Proposition : Inversibilit√© de $X'X$

Soit $X \in \mathbb{R}^{n \times p}$. Alors $X'X$ est inversible si et
seulement si $X$ est de rang plein, c'est-√†-dire
$\operatorname{rang}(X) = p$.

**Preuve**

[Sens direct ($\Rightarrow$)]{.underline}

Supposons que $X' X$ soit inversible.\
Si $\operatorname{rang}(X) < p$, alors il existe
$v \in \mathbb{R}^p \setminus \{0\}$ tel que $Xv = 0$.\
On aurait alors 
$$
X^\top X v = X^\top (Xv) = X^\top 0 = 0,
$$

ce qui montre que $v$ est un vecteur non nul du noyau de $X'X$.\
C'est absurde car cela contredit l‚Äôinversibilit√© de $X'X$. Donc
n√©cessairement $\operatorname{rang}(X) = p$.

[Sens r√©ciproque ($\Leftarrow$)]{.underline}

Supposons maintenant que $\operatorname{rang}(X) = p$, i.e. les colonnes
de $X$ sont lin√©airement ind√©pendantes.\
Prenons $v \in \mathbb{R}^p$ tel que $X^\top X v = 0$.\
Alors :
$$
0 = v' (X' X v) = (Xv)'(Xv) = \|Xv\|_2^2
$$
Ainsi, $Xv = 0$. Or, comme $X$ est de rang plein, son noyau est
r√©duit √† $\{0\}$, donc $v=0$.\
Cela montre que le noyau de $X' X$ est trivial, donc $X' X$ est
inversible.
:::

:::

:::

## Question 4

Les variables $y_i$ ont-elles m√™me esp√©rance ?

::: {.content-visible when-meta="is_answer_print_4"}

::: answer
Calculons l'esp√©rance de la variable al√©atoire $y_i$. on suppose les
$x_i$ d√©terministes pour simplifier la notation sinon il faut raisonner
avec des esp√©rances conditonnelles et supposer que les variables
$\epsilon_i$ et $x_i$ sont ind√©pendantes quel que soit
$i \in [\![1,n]\!]$.

Par lin√©arit√© de l'esp√©rance, on a :
$$\mathbb{E}(y_i)=\mathbb{E}(\beta_0+\beta_1x_i+\epsilon_i)=\mathbb{E}(\beta_0)+\mathbb{E}(\beta_1x_i)+\mathbb{E}(\epsilon_i)$$
Or

-   $\mathbb{E}(\beta_0)=\beta_0$ et $\mathbb{E}(\beta_1x_i)=\beta_1x_i$
    car $\beta_0$, $\beta_1$ et $x_i$ sont d√©terministes ;
-   $\mathbb{E}(\epsilon_i)=0$ par hypoth√®se du mod√®le.

Donc $$\mathbb{E}(y_i)=\beta_0+\beta_1x_i$$

Les variables $y_i$ n'ont pas la m√™me esp√©rance sauf si $\beta_1=0$.
:::

:::

## Question 5

La droite de r√©gression estim√©e √† partir des observations
$(x_i; y_i)_{i \in [\![1,n]\!]}$ passe-t-elle toujours par le point
$(\overline{x}; \overline{y})$ ?

::: {.content-visible when-meta="is_answer_print_5"}

::: answer
Oui (*cf question 5 de l'exercice 3*)
:::

:::

## Question 6

Les estimateurs par MCO des coefficients $\hat{\beta}_0$
et $\hat{\beta}_1$ sont-ils ind√©pendants ?

::: {.content-visible when-meta="is_answer_print_6"}

::: answer
Montrons que les estimateurs par MCO des coefficients $\hat{\beta}_0$
et$\hat{\beta}_1$ ne sont pas ind√©pendants. √âtudions la matrice de
variance-covariance de $\hat{\beta}$ :

$$\mathbb{V}(\hat{\beta})=\sigma^2(X'X)^{-1}=\frac{\sigma^2}{n\textrm{var}(x)}\begin{pmatrix}
\overline{x^2} & -\overline{x} \\
-\overline{x} & 1
\end{pmatrix}$$

Cette matrice n'est pas n√©cessairement diagonale (il faut que
$\overline{x}=0$) et m√™me dans ce cas cela ne garantit pas
l‚Äôind√©pendance sans hypoth√®se gaussienne.
:::

:::

## Question 7

Est-il possible de trouver des estimateurs des coefficients de
r√©gression de plus faible variance que celle des estimateurs par MCO ?

::: {.content-visible when-meta="is_answer_print_7"}

::: answer
Oui, en particulier si on accepte du biais, on peut trouver des
estimateurs de variance plus faible.

On peut consid√©rer $\hat{\beta}= \frac{1}{2}\hat{\beta}_{\textrm{MCO}}$
par exemple.
:::

:::
